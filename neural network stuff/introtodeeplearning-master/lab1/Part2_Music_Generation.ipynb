{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Part2_Music_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uoJsVjtCMunI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.9.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "8aa8362771f09199fa2555da6b05733ce08ed364b1b18a9023dfc7c3066aed74"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab1/Part2_Music_Generation.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/aamini/introtodeeplearning/blob/master/lab1/Part2_Music_Generation.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ],
      "metadata": {
        "id": "uoJsVjtCMunI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "source": [
        "# Copyright 2021 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved.\r\n",
        "# \r\n",
        "# Licensed under the MIT License. You may not use this file except in compliance\r\n",
        "# with the License. Use and/or modification of this code outside of 6.S191 must\r\n",
        "# reference:\r\n",
        "#\r\n",
        "# Â© MIT 6.S191: Introduction to Deep Learning\r\n",
        "# http://introtodeeplearning.com\r\n",
        "#"
      ],
      "outputs": [],
      "metadata": {
        "id": "bUik05YqMyCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1: Intro to TensorFlow and Music Generation with RNNs\n",
        "\n",
        "# Part 2: Music Generation with RNNs\n",
        "\n",
        "In this portion of the lab, we will explore building a Recurrent Neural Network (RNN) for music generation. We will train a model to learn the patterns in raw sheet music in [ABC notation](https://en.wikipedia.org/wiki/ABC_notation) and then use this model to generate new music. "
      ],
      "metadata": {
        "id": "O-97SDET3JG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Dependencies \n",
        "First, let's download the course repository, install dependencies, and import the relevant packages we'll need for this lab."
      ],
      "metadata": {
        "id": "rsvlBQYCrE4I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "source": [
        "# Import Tensorflow 2.0\r\n",
        "# %tensorflow_version 2.x\r\n",
        "import tensorflow as tf \r\n",
        "\r\n",
        "# Download and import the MIT 6.S191 package\r\n",
        "# !pip install mitdeeplearning\r\n",
        "import mitdeeplearning as mdl\r\n",
        "\r\n",
        "# Import all remaining packages\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import time\r\n",
        "import functools\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "from tqdm import tqdm\r\n",
        "# !apt-get install abcmidi timidity > /dev/null 2>&1\r\n",
        "\r\n",
        "# Check that we are using a GPU, if not switch runtimes\r\n",
        "#   using Runtime > Change Runtime Type > GPU\r\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0"
      ],
      "outputs": [],
      "metadata": {
        "id": "riVZCVK65QTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Dataset\n",
        "\n",
        "![Let's Dance!](http://33.media.tumblr.com/3d223954ad0a77f4e98a7b87136aa395/tumblr_nlct5lFVbF1qhu7oio1_500.gif)\n",
        "\n",
        "We've gathered a dataset of thousands of Irish folk songs, represented in the ABC notation. Let's download the dataset and inspect it: \n"
      ],
      "metadata": {
        "id": "_ajvp0No4qDm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "source": [
        "# Download the dataset\r\n",
        "songs = mdl.lab1.load_training_data()\r\n",
        "\r\n",
        "# Print one of the songs to inspect it in greater detail!\r\n",
        "example_song = songs[0]\r\n",
        "print(\"\\nExample song: \")\r\n",
        "print(example_song)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 817 songs in text\n",
            "\n",
            "Example song: \n",
            "X:1\n",
            "T:Alexander's\n",
            "Z: id:dc-hornpipe-1\n",
            "M:C|\n",
            "L:1/8\n",
            "K:D Major\n",
            "(3ABc|dAFA DFAd|fdcd FAdf|gfge fefd|(3efe (3dcB A2 (3ABc|!\n",
            "dAFA DFAd|fdcd FAdf|gfge fefd|(3efe dc d2:|!\n",
            "AG|FAdA FAdA|GBdB GBdB|Acec Acec|dfaf gecA|!\n",
            "FAdA FAdA|GBdB GBdB|Aceg fefd|(3efe dc d2:|!\n"
          ]
        }
      ],
      "metadata": {
        "id": "P7dFnP5q3Jve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily convert a song in ABC notation to an audio waveform and play it back. Be patient for this conversion to run, it can take some time."
      ],
      "metadata": {
        "id": "hKF3EHJlCAj2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "source": [
        "# Convert the ABC notation to audio file and listen to it\r\n",
        "mdl.lab1.play_song(example_song)"
      ],
      "outputs": [],
      "metadata": {
        "id": "11toYzhEEKDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important thing to think about is that this notation of music does not simply contain information on the notes being played, but additionally there is meta information such as the song title, key, and tempo. How does the number of different characters that are present in the text file impact the complexity of the learning problem? This will become important soon, when we generate a numerical representation for the text data."
      ],
      "metadata": {
        "id": "7vH24yyquwKQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "source": [
        "# Join our list of song strings into a single string containing all songs\r\n",
        "songs_joined = \"\\n\\n\".join(songs) \r\n",
        "\r\n",
        "# Find all unique characters in the joined string\r\n",
        "vocab = sorted(set(songs_joined))\r\n",
        "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 83 unique characters in the dataset\n"
          ]
        }
      ],
      "metadata": {
        "id": "IlCgQBRVymwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Process the dataset for the learning task\n",
        "\n",
        "Let's take a step back and consider our prediction task. We're trying to train a RNN model to learn patterns in ABC music, and then use this model to generate (i.e., predict) a new piece of music based on this learned information. \n",
        "\n",
        "Breaking this down, what we're really asking the model is: given a character, or a sequence of characters, what is the most probable next character? We'll train the model to perform this task. \n",
        "\n",
        "To achieve this, we will input a sequence of characters to the model, and train the model to predict the output, that is, the following character at each time step. RNNs maintain an internal state that depends on previously seen elements, so information about all characters seen up until a given moment will be taken into account in generating the prediction."
      ],
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before we begin training our RNN model, we'll need to create a numerical representation of our text-based dataset. To do this, we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters. Recall that we just identified the unique characters present in the text."
      ],
      "metadata": {
        "id": "LFjSVAlWzf-N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "### Define numerical representation of text ###\r\n",
        "\r\n",
        "# Create a mapping from character to unique index.\r\n",
        "# For example, to get the index of the character \"d\", \r\n",
        "#   we can evaluate `char2idx[\"d\"]`.  \r\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\r\n",
        "\r\n",
        "# Create a mapping from indices to characters. This is\r\n",
        "#   the inverse of char2idx and allows us to convert back\r\n",
        "#   from unique index to the character in our vocabulary.\r\n",
        "idx2char = np.array(vocab)"
      ],
      "outputs": [],
      "metadata": {
        "id": "IalZLbvOzf-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us an integer representation for each character. Observe that the unique characters (i.e., our vocabulary) in the text are mapped as indices from 0 to `len(unique)`. Let's take a peek at this numerical representation of our dataset:"
      ],
      "metadata": {
        "id": "tZfqhkYCymwX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "source": [
        "print('{')\r\n",
        "for char,_ in zip(char2idx, range(20)):\r\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\r\n",
        "print('  ...\\n}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '\"' :   3,\n",
            "  '#' :   4,\n",
            "  \"'\" :   5,\n",
            "  '(' :   6,\n",
            "  ')' :   7,\n",
            "  ',' :   8,\n",
            "  '-' :   9,\n",
            "  '.' :  10,\n",
            "  '/' :  11,\n",
            "  '0' :  12,\n",
            "  '1' :  13,\n",
            "  '2' :  14,\n",
            "  '3' :  15,\n",
            "  '4' :  16,\n",
            "  '5' :  17,\n",
            "  '6' :  18,\n",
            "  '7' :  19,\n",
            "  ...\n",
            "}\n"
          ]
        }
      ],
      "metadata": {
        "id": "FYyNlCNXymwY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "source": [
        "### Vectorize the songs string ###\r\n",
        "\r\n",
        "'''TODO: Write a function to convert the all songs string to a vectorized\r\n",
        "    (i.e., numeric) representation. Use the appropriate mapping\r\n",
        "    above to convert from vocab characters to the corresponding indices.\r\n",
        "\r\n",
        "  NOTE: the output of the `vectorize_string` function \r\n",
        "  should be a np.array with `N` elements, where `N` is\r\n",
        "  the number of characters in the input string\r\n",
        "'''\r\n",
        "\r\n",
        "def vectorize_string(string):\r\n",
        "    return np.array([char2idx[i] for i in string])\r\n",
        "\r\n",
        "vectorized_songs = vectorize_string(songs_joined)"
      ],
      "outputs": [],
      "metadata": {
        "id": "g-LnKyu4dczc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at how the first part of the text is mapped to an integer representation:"
      ],
      "metadata": {
        "id": "IqxpSuZ1w-ub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "source": [
        "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\r\n",
        "# check that vectorized_songs is a numpy array\r\n",
        "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'X:1\\nT:Alex' ---- characters mapped to int ----> [49 22 13  0 45 22 26 67 60 79]\n"
          ]
        }
      ],
      "metadata": {
        "id": "l1VKcQHcymwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Our next step is to actually divide the text into example sequences that we'll use during training. Each input sequence that we feed into our RNN will contain `seq_length` characters from the text. We'll also need to define a target sequence for each input sequence, which will be used in training the RNN to predict the next character. For each input, the corresponding target will contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "To do this, we'll break the text into chunks of `seq_length+1`. Suppose `seq_length` is 4 and our text is \"Hello\". Then, our input sequence is \"Hell\" and the target sequence is \"ello\".\n",
        "\n",
        "The batch method will then let us convert this stream of character indices to sequences of the desired size."
      ],
      "metadata": {
        "id": "hgsVvVxnymwf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "source": [
        "### Batch definition to create training examples ###\r\n",
        "\r\n",
        "def get_batch(vectorized_songs, seq_length, batch_size):\r\n",
        "  # the length of the vectorized songs string\r\n",
        "  n = vectorized_songs.shape[0] - 1\r\n",
        "  # randomly choose the starting indices for the examples in the training batch\r\n",
        "  idx = np.random.choice(n-seq_length, batch_size)\r\n",
        "\r\n",
        "  '''TODO: construct a list of input sequences for the training batch'''\r\n",
        "  input_batch = np.array([vectorized_songs[i:i+seq_length] for i in idx])\r\n",
        "  '''TODO: construct a list of output sequences for the training batch'''\r\n",
        "  output_batch = np.array([vectorized_songs[i+1:i+seq_length+1] for i in idx])\r\n",
        "\r\n",
        "  # x_batch, y_batch provide the true inputs and targets for network training\r\n",
        "  x_batch = np.reshape(input_batch, [batch_size, seq_length])\r\n",
        "  y_batch = np.reshape(output_batch, [batch_size, seq_length])\r\n",
        "  return x_batch, y_batch\r\n",
        "\r\n",
        "\r\n",
        "# Perform some simple tests to make sure your batch function is working properly! \r\n",
        "test_args = (vectorized_songs, 10, 2)\r\n",
        "if not mdl.lab1.test_batch_func_types(get_batch, test_args) or \\\r\n",
        "   not mdl.lab1.test_batch_func_shapes(get_batch, test_args) or \\\r\n",
        "   not mdl.lab1.test_batch_func_next_step(get_batch, test_args): \r\n",
        "   print(\"======\\n[FAIL] could not pass tests\")\r\n",
        "else: \r\n",
        "   print(\"======\\n[PASS] passed all tests!\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PASS] test_batch_func_types\n",
            "[PASS] test_batch_func_shapes\n",
            "[PASS] test_batch_func_next_step\n",
            "======\n",
            "[PASS] passed all tests!\n"
          ]
        }
      ],
      "metadata": {
        "id": "LF-N8F7BoDRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each of these vectors, each index is processed at a single time step. So, for the input at time step 0, the model receives the index for the first character in the sequence, and tries to predict the index of the next character. At the next timestep, it does the same thing, but the RNN considers the information from the previous step, i.e., its updated state, in addition to the current input.\n",
        "\n",
        "We can make this concrete by taking a look at how this works over the first several characters in our text:"
      ],
      "metadata": {
        "id": "_33OHL3b84i0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "source": [
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\r\n",
        "\r\n",
        "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\r\n",
        "    print(\"Step {:3d}\".format(i))\r\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\r\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step   0\n",
            "  input: 62 ('g')\n",
            "  expected output: 60 ('e')\n",
            "Step   1\n",
            "  input: 60 ('e')\n",
            "  expected output: 61 ('f')\n",
            "Step   2\n",
            "  input: 61 ('f')\n",
            "  expected output: 1 (' ')\n",
            "Step   3\n",
            "  input: 1 (' ')\n",
            "  expected output: 59 ('d')\n",
            "Step   4\n",
            "  input: 59 ('d')\n",
            "  expected output: 14 ('2')\n"
          ]
        }
      ],
      "metadata": {
        "id": "0eBu9WZG84i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 The Recurrent Neural Network (RNN) model"
      ],
      "metadata": {
        "id": "r6oUuElIMgVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to define and train a RNN model on our ABC music dataset, and then use that trained model to generate a new song. We'll train our RNN using batches of song snippets from our dataset, which we generated in the previous section.\n",
        "\n",
        "The model is based off the LSTM architecture, where we use a state vector to maintain information about the temporal relationships between consecutive characters. The final output of the LSTM is then fed into a fully connected [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer where we'll output a softmax over each character in the vocabulary, and then sample from this distribution to predict the next character. \n",
        "\n",
        "As we introduced in the first portion of this lab, we'll be using the Keras API, specifically, [`tf.keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential), to define the model. Three layers are used to define the model:\n",
        "\n",
        "* [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): This is the input layer, consisting of a trainable lookup table that maps the numbers of each character to a vector with `embedding_dim` dimensions.\n",
        "* [`tf.keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Our LSTM network, with size `units=rnn_units`. \n",
        "* [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): The output layer, with `vocab_size` outputs.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab1/img/lstm_unrolled-01-01.png\" alt=\"Drawing\"/>"
      ],
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the RNN model\n",
        "\n",
        "Now, we will define a function that we will use to actually build the model."
      ],
      "metadata": {
        "id": "rlaOqndqBmJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "source": [
        "def LSTM(rnn_units): \r\n",
        "  return tf.keras.layers.LSTM(\r\n",
        "    rnn_units, \r\n",
        "    return_sequences=True, \r\n",
        "    recurrent_initializer='glorot_uniform',\r\n",
        "    recurrent_activation='sigmoid',\r\n",
        "    stateful=True,\r\n",
        "  )"
      ],
      "outputs": [],
      "metadata": {
        "id": "8DsWzojvkbc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The time has come! Fill in the `TODOs` to define the RNN model within the `build_model` function, and then call the function you just defined to instantiate the model!"
      ],
      "metadata": {
        "id": "IbWU4dMJmMvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "source": [
        "### Defining the RNN Model ###\r\n",
        "\r\n",
        "'''TODO: Add LSTM and Dense layers to define the RNN model using the Sequential API.'''\r\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n",
        "  model = tf.keras.Sequential([\r\n",
        "    # Layer 1: Embedding layer to transform indices into dense vectors \r\n",
        "    #   of a fixed embedding size\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\r\n",
        "\r\n",
        "    # Layer 2: LSTM with `rnn_units` number of units. \r\n",
        "    # TODO: Call the LSTM function defined above to add this layer.\r\n",
        "    LSTM(rnn_units),\r\n",
        "\r\n",
        "    # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\r\n",
        "    #   into the vocabulary size. \r\n",
        "    # TODO: Add the Dense layer.\r\n",
        "    tf.keras.layers.Dense(vocab_size)\r\n",
        "  ])\r\n",
        "\r\n",
        "  return model\r\n",
        "\r\n",
        "# Build a simple model with default hyperparameters. You will get the \r\n",
        "#   chance to change these later.\r\n",
        "model = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MtCrdfzEI2N0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test out the RNN model\n",
        "\n",
        "It's always a good idea to run a few simple checks on our model to see that it behaves as expected.  \n",
        "\n",
        "First, we can use the `Model.summary` function to print out a summary of our model's internal workings. Here we can check the layers in the model, the shape of the output of each of the layers, the batch size, etc."
      ],
      "metadata": {
        "id": "-ubPo0_9Prjb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "source": [
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (32, None, 256)           21248     \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (32, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (32, None, 83)            85075     \n",
            "=================================================================\n",
            "Total params: 5,353,299\n",
            "Trainable params: 5,353,299\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "RwG1DD6rDrRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also quickly check the dimensionality of our output, using a sequence length of 100. Note that the model can be run on inputs of any length."
      ],
      "metadata": {
        "id": "8xeDn5nZD0LX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "source": [
        "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\r\n",
        "pred = model(x)\r\n",
        "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\r\n",
        "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:       (32, 100)  # (batch_size, sequence_length)\n",
            "Prediction shape:  (32, 100, 83) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "metadata": {
        "id": "C-_70kKAPrPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions from the untrained model\n",
        "\n",
        "Let's take a look at what our untrained model is predicting.\n",
        "\n",
        "To get actual predictions from the model, we sample from the output distribution, which is defined by a `softmax` over our character vocabulary. This will give us actual character indices. This means we are using a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) to sample over the example prediction. This gives a prediction of the next character (specifically its index) at each timestep.\n",
        "\n",
        "Note here that we sample from this probability distribution, as opposed to simply taking the `argmax`, which can cause the model to get stuck in a loop.\n",
        "\n",
        "Let's try this sampling out for the first example in the batch."
      ],
      "metadata": {
        "id": "mT1HvFVUGpoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "source": [
        "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\r\n",
        "sampled_indices"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([81, 42, 67, 72, 29, 63, 67, 10, 61, 36,  3, 65, 70, 56, 55, 33, 21,\n",
              "       39, 15, 81, 75, 11, 60,  6, 35, 38,  5, 28, 59, 48, 19, 33, 38, 81,\n",
              "       31, 76,  7,  9, 52, 41, 33, 15, 52, 58, 77, 38, 40, 71, 38,  9, 59,\n",
              "       72, 59, 10, 70,  0, 29, 40, 58, 58, 48, 47, 66,  0, 68, 60,  1, 56,\n",
              "       49, 14, 38, 39, 43, 77, 29, 16, 18, 69, 74, 29, 64, 43, 52, 82, 30,\n",
              "       78, 49,  3,  9,  3, 58, 31, 37,  7, 12, 57, 59, 42, 54, 67],\n",
              "      dtype=int64)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "metadata": {
        "id": "4V4MfFg0RQJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now decode these to see the text predicted by the untrained model:"
      ],
      "metadata": {
        "id": "LfLtsP3mUhCG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\r\n",
        "print()\r\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            " 'ba|gedB G3:|!\\nA|Bdd2 edd2|gdfd edgd|Bdef g2bg|gedB A2GA|!\\nBdd2 edd2|gdfd edgd|Bdef g2bg|gedB G3:|!\\n\\n'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'zQlqDhl.fK\"joa_H9N3zt/e(JM\\'CdW7HMzFu)-[PH3[cvMOpM-dqd.o\\nDOccWVk\\nme aX2MNRvD46nsDiR[|EwX\"-\"cFL)0bdQ^l'\n"
          ]
        }
      ],
      "metadata": {
        "id": "xWcFwPwLSo05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the text predicted by the untrained model is pretty nonsensical! How can we do better? We can train the network!"
      ],
      "metadata": {
        "id": "HEHHcRasIDm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Training the model: loss and training operations\n",
        "\n",
        "Now it's time to train the model!\n",
        "\n",
        "At this point, we can think of our next character prediction problem as a standard classification problem. Given the previous state of the RNN, as well as the input at a given time step, we want to predict the class of the next character -- that is, to actually predict the next character. \n",
        "\n",
        "To train our model on this classification task, we can use a form of the `crossentropy` loss (negative log likelihood loss). Specifically, we will use the [`sparse_categorical_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy) loss, as it utilizes integer targets for categorical classification tasks. We will want to compute the loss using the true targets -- the `labels` -- and the predicted targets -- the `logits`.\n",
        "\n",
        "Let's first compute the loss using our example predictions from the untrained model: "
      ],
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "source": [
        "### Defining the loss function ###\r\n",
        "\r\n",
        "'''TODO: define the loss function to compute and return the loss between\r\n",
        "    the true labels and predictions (logits). Set the argument from_logits=True.'''\r\n",
        "def compute_loss(labels, logits):\r\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True) # TODO\r\n",
        "  return loss\r\n",
        "\r\n",
        "'''TODO: compute the loss using the true next characters from the example batch \r\n",
        "    and the predictions from the untrained model several cells above'''\r\n",
        "example_batch_loss = compute_loss(y,pred) # TODO\r\n",
        "\r\n",
        "print(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\") \r\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (32, 100, 83)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.4187975\n"
          ]
        }
      ],
      "metadata": {
        "id": "4HrXTACTdzY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by defining some hyperparameters for training the model. To start, we have provided some reasonable values for some of the parameters. It is up to you to use what we've learned in class to help optimize the parameter selection here!"
      ],
      "metadata": {
        "id": "0Seh7e6eRqd7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "source": [
        "### Hyperparameter setting and optimization ###\r\n",
        "\r\n",
        "# Optimization parameters:\r\n",
        "num_training_iterations = 1500  # Increase this to train longer\r\n",
        "batch_size = 50  # Experiment between 1 and 64\r\n",
        "seq_length = 300  # Experiment between 50 and 500\r\n",
        "learning_rate = 1e-3  # Experiment between 1e-5 and 1e-1\r\n",
        "\r\n",
        "# Model parameters: \r\n",
        "vocab_size = len(vocab)\r\n",
        "embedding_dim = 256 \r\n",
        "rnn_units = 1024  # Experiment between 1 and 2048\r\n",
        "\r\n",
        "# Checkpoint location: \r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "JQWUUhKotkAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are ready to define our training operation -- the optimizer and duration of training -- and use this function to train the model. You will experiment with the choice of optimizer and the duration for which you train your models, and see how these changes affect the network's output. Some optimizers you may like to try are [`Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam?version=stable) and [`Adagrad`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad?version=stable).\n",
        "\n",
        "First, we will instantiate a new model and an optimizer. Then, we will use the [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape) method to perform the backpropagation operations. \n",
        "\n",
        "We will also generate a print-out of the model's progress through training, which will help us easily visualize whether or not we are minimizing the loss."
      ],
      "metadata": {
        "id": "5cu11p1MKYZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "source": [
        "### Define optimizer and training operation ###\r\n",
        "\r\n",
        "'''TODO: instantiate a new model for training using the `build_model`\r\n",
        "  function and the hyperparameters created above.'''\r\n",
        "model = build_model(vocab_size,embedding_dim,rnn_units,batch_size)\r\n",
        "\r\n",
        "'''TODO: instantiate an optimizer with its learning rate.\r\n",
        "  Checkout the tensorflow website for a list of supported optimizers.\r\n",
        "  https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/\r\n",
        "  Try using the Adam optimizer to start.'''\r\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(x, y): \r\n",
        "  # Use tf.GradientTape()\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "  \r\n",
        "    '''TODO: feed the current input into the model and generate predictions'''\r\n",
        "    y_hat = model(x)\r\n",
        "  \r\n",
        "    '''TODO: compute the loss!'''\r\n",
        "    loss = compute_loss(y, y_hat)\r\n",
        "\r\n",
        "  # Now, compute the gradients \r\n",
        "  '''TODO: complete the function call for gradient computation. \r\n",
        "      Remember that we want the gradient of the loss with respect all \r\n",
        "      of the model parameters. \r\n",
        "      HINT: use `model.trainable_variables` to get a list of all model\r\n",
        "      parameters.'''\r\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\r\n",
        "  \r\n",
        "  # Apply the gradients to the optimizer so it can update the model accordingly\r\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n",
        "  return loss\r\n",
        "\r\n",
        "##################\r\n",
        "# Begin training!#\r\n",
        "##################\r\n",
        "\r\n",
        "history = []\r\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\r\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\r\n",
        "\r\n",
        "for iter in tqdm(range(num_training_iterations)):\r\n",
        "\r\n",
        "  # Grab a batch and propagate it through the network\r\n",
        "  x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\r\n",
        "  loss = train_step(x_batch, y_batch)\r\n",
        "\r\n",
        "  # Update the progress bar\r\n",
        "  history.append(loss.numpy().mean())\r\n",
        "  plotter.plot(history)\r\n",
        "\r\n",
        "  # Update the model with the changed weights!\r\n",
        "  if iter % 100 == 0:     \r\n",
        "    model.save_weights(checkpoint_prefix)\r\n",
        "    \r\n",
        "# Save the trained model and the weights\r\n",
        "model.save_weights(checkpoint_prefix)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 376.511593 262.19625\" width=\"376.511593pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-31T21:03:01.404859</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 376.511593 262.19625 \r\nL 376.511593 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 34.240625 224.64 \r\nL 369.040625 224.64 \r\nL 369.040625 7.2 \r\nL 34.240625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m00bbc7b011\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.458807\" xlink:href=\"#m00bbc7b011\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(46.277557 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"110.884364\" xlink:href=\"#m00bbc7b011\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(101.340614 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"172.309921\" xlink:href=\"#m00bbc7b011\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(162.766171 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"233.735479\" xlink:href=\"#m00bbc7b011\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(224.191729 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"295.161036\" xlink:href=\"#m00bbc7b011\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(285.617286 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.586593\" xlink:href=\"#m00bbc7b011\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(343.861593 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Iterations -->\r\n     <g transform=\"translate(177.827344 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 1259 4666 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-49\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3597 1894 \r\nL 3597 1613 \r\nL 953 1613 \r\nQ 991 1019 1311 708 \r\nQ 1631 397 2203 397 \r\nQ 2534 397 2845 478 \r\nQ 3156 559 3463 722 \r\nL 3463 178 \r\nQ 3153 47 2828 -22 \r\nQ 2503 -91 2169 -91 \r\nQ 1331 -91 842 396 \r\nQ 353 884 353 1716 \r\nQ 353 2575 817 3079 \r\nQ 1281 3584 2069 3584 \r\nQ 2775 3584 3186 3129 \r\nQ 3597 2675 3597 1894 \r\nz\r\nM 3022 2063 \r\nQ 3016 2534 2758 2815 \r\nQ 2500 3097 2075 3097 \r\nQ 1594 3097 1305 2825 \r\nQ 1016 2553 972 2059 \r\nL 3022 2063 \r\nz\r\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-49\"/>\r\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"68.701172\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"130.224609\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"171.337891\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"232.617188\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"271.826172\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"299.609375\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"360.791016\" xlink:href=\"#DejaVuSans-6e\"/>\r\n      <use x=\"424.169922\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m77145069d8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m77145069d8\" y=\"181.197047\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(20.878125 184.996265)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m77145069d8\" y=\"133.209158\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(20.878125 137.008377)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m77145069d8\" y=\"85.22127\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(20.878125 89.020489)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m77145069d8\" y=\"37.233382\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(20.878125 41.0326)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_12\">\r\n     <!-- Loss -->\r\n     <g transform=\"translate(14.798437 126.887187)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 1259 4666 \r\nL 1259 531 \r\nL 3531 531 \r\nL 3531 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-4c\"/>\r\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_11\">\r\n    <path clip-path=\"url(#p9654f745d8)\" d=\"M 49.458807 17.083636 \r\nL 49.765935 18.880775 \r\nL 50.073062 42.160353 \r\nL 50.38019 33.436801 \r\nL 50.994446 52.265465 \r\nL 51.608701 64.901222 \r\nL 51.915829 66.087847 \r\nL 52.222957 62.530182 \r\nL 52.530085 65.675449 \r\nL 52.837212 66.531182 \r\nL 53.14434 66.28102 \r\nL 53.451468 66.838722 \r\nL 53.758596 67.829724 \r\nL 54.372851 66.789341 \r\nL 54.679979 66.634347 \r\nL 54.987107 68.154758 \r\nL 55.294235 67.99116 \r\nL 55.601363 69.530334 \r\nL 55.90849 67.899653 \r\nL 56.215618 68.652255 \r\nL 56.522746 68.68357 \r\nL 57.137001 71.194731 \r\nL 57.444129 71.170121 \r\nL 58.058385 72.360395 \r\nL 58.979768 73.895828 \r\nL 59.286896 74.575927 \r\nL 59.594024 79.179639 \r\nL 59.901152 77.828113 \r\nL 61.129663 85.074159 \r\nL 61.43679 86.212067 \r\nL 61.743918 86.22384 \r\nL 62.051046 87.09955 \r\nL 62.358174 90.133631 \r\nL 62.665302 89.391555 \r\nL 62.972429 91.165044 \r\nL 63.279557 92.153175 \r\nL 63.586685 94.167617 \r\nL 63.893813 94.275896 \r\nL 64.200941 96.930164 \r\nL 64.508068 98.425117 \r\nL 64.815196 98.829839 \r\nL 65.122324 99.786518 \r\nL 65.429452 101.422633 \r\nL 65.736579 101.553452 \r\nL 66.657963 108.502244 \r\nL 66.965091 104.954555 \r\nL 68.193602 114.478763 \r\nL 68.50073 114.630794 \r\nL 68.807857 118.506913 \r\nL 69.114985 116.813203 \r\nL 69.422113 119.400941 \r\nL 69.729241 120.106028 \r\nL 70.036369 121.884952 \r\nL 70.343496 122.138695 \r\nL 70.650624 126.182405 \r\nL 70.957752 126.397843 \r\nL 71.26488 128.107194 \r\nL 72.186263 129.785103 \r\nL 72.493391 132.832411 \r\nL 72.800519 131.232381 \r\nL 73.107646 134.515852 \r\nL 73.414774 133.898571 \r\nL 73.721902 134.999993 \r\nL 74.02903 137.188689 \r\nL 74.643285 136.805815 \r\nL 75.257541 138.430443 \r\nL 75.564669 140.09908 \r\nL 75.871796 139.733362 \r\nL 76.178924 143.306261 \r\nL 76.486052 141.417622 \r\nL 76.79318 140.765158 \r\nL 77.407435 145.39364 \r\nL 77.714563 145.522325 \r\nL 78.021691 146.227521 \r\nL 78.635947 143.532648 \r\nL 78.943074 145.243909 \r\nL 79.250202 144.883935 \r\nL 79.55733 148.687878 \r\nL 79.864458 147.506848 \r\nL 80.171585 145.75838 \r\nL 80.478713 148.924853 \r\nL 80.785841 148.597955 \r\nL 81.092969 148.521711 \r\nL 81.707224 150.773763 \r\nL 82.014352 150.184759 \r\nL 82.32148 150.50549 \r\nL 82.628608 151.770109 \r\nL 82.935736 150.373379 \r\nL 83.242863 152.07698 \r\nL 83.549991 150.985157 \r\nL 83.857119 153.80808 \r\nL 84.164247 152.848105 \r\nL 84.471374 152.805475 \r\nL 84.778502 152.999209 \r\nL 85.08563 152.363662 \r\nL 85.699886 154.376416 \r\nL 86.007013 154.667371 \r\nL 86.621269 158.012693 \r\nL 86.928397 154.289703 \r\nL 87.235525 152.612811 \r\nL 88.156908 156.402218 \r\nL 88.464036 153.94983 \r\nL 88.771163 155.111433 \r\nL 89.078291 154.609032 \r\nL 89.385419 156.005248 \r\nL 89.692547 155.178587 \r\nL 90.306802 155.39097 \r\nL 90.921058 158.107587 \r\nL 91.535314 155.809993 \r\nL 91.842441 155.758873 \r\nL 92.149569 158.219671 \r\nL 92.456697 157.464368 \r\nL 92.763825 155.91782 \r\nL 93.070952 157.049052 \r\nL 93.37808 159.852084 \r\nL 93.685208 157.821734 \r\nL 93.992336 158.006561 \r\nL 94.299464 158.913465 \r\nL 94.606591 155.288583 \r\nL 94.913719 159.86241 \r\nL 95.220847 157.878831 \r\nL 95.527975 158.191817 \r\nL 96.449358 159.514454 \r\nL 96.756486 159.125482 \r\nL 97.063614 159.575425 \r\nL 97.370742 159.405357 \r\nL 97.677869 161.98576 \r\nL 97.984997 160.262806 \r\nL 98.292125 159.294612 \r\nL 98.599253 159.054758 \r\nL 98.90638 160.242178 \r\nL 99.213508 159.959832 \r\nL 99.520636 161.031118 \r\nL 99.827764 158.835368 \r\nL 100.442019 162.278724 \r\nL 100.749147 159.660856 \r\nL 101.056275 160.884951 \r\nL 101.363403 159.919582 \r\nL 101.670531 161.919075 \r\nL 101.977658 162.585297 \r\nL 102.284786 162.150897 \r\nL 102.591914 163.669139 \r\nL 103.206169 161.698763 \r\nL 103.513297 162.61402 \r\nL 103.820425 162.726607 \r\nL 104.127553 161.7686 \r\nL 104.434681 159.824517 \r\nL 104.741808 162.869399 \r\nL 105.048936 164.046687 \r\nL 105.663192 160.719351 \r\nL 105.97032 162.860486 \r\nL 106.277447 162.679521 \r\nL 106.584575 164.214821 \r\nL 107.198831 163.435881 \r\nL 107.505958 164.016328 \r\nL 107.813086 163.541427 \r\nL 108.120214 162.661278 \r\nL 108.427342 163.800564 \r\nL 108.73447 164.493712 \r\nL 109.041597 163.970283 \r\nL 109.348725 163.045616 \r\nL 109.655853 164.495411 \r\nL 109.962981 165.147468 \r\nL 110.577236 164.642917 \r\nL 110.884364 165.506333 \r\nL 111.49862 164.36243 \r\nL 111.805747 164.264185 \r\nL 112.112875 165.787569 \r\nL 112.420003 165.286599 \r\nL 112.727131 166.61617 \r\nL 113.034259 165.292205 \r\nL 113.341386 166.527787 \r\nL 113.648514 166.434118 \r\nL 113.955642 166.021862 \r\nL 114.26277 168.560282 \r\nL 114.569898 165.490407 \r\nL 114.877025 166.707213 \r\nL 115.184153 165.448795 \r\nL 115.798409 167.649528 \r\nL 116.105536 166.573237 \r\nL 116.412664 166.634733 \r\nL 116.719792 166.073296 \r\nL 117.02692 168.2085 \r\nL 117.334048 166.286772 \r\nL 117.641175 167.748889 \r\nL 117.948303 166.826099 \r\nL 118.255431 166.376877 \r\nL 118.562559 165.483748 \r\nL 118.869687 166.802868 \r\nL 119.176814 165.786574 \r\nL 119.483942 166.920952 \r\nL 119.79107 165.138247 \r\nL 120.712453 167.948698 \r\nL 121.019581 168.212207 \r\nL 121.326709 169.107641 \r\nL 121.633837 166.460593 \r\nL 121.940964 168.542743 \r\nL 122.248092 168.154732 \r\nL 122.55522 168.039982 \r\nL 122.862348 166.755947 \r\nL 123.169476 169.162381 \r\nL 123.476603 169.412675 \r\nL 123.783731 166.810442 \r\nL 124.090859 168.572216 \r\nL 124.397987 168.716632 \r\nL 124.705114 168.063986 \r\nL 125.012242 169.328788 \r\nL 125.31937 168.254099 \r\nL 125.626498 167.874085 \r\nL 125.933626 168.254059 \r\nL 126.240753 168.896362 \r\nL 126.547881 170.343034 \r\nL 126.855009 169.769858 \r\nL 127.162137 169.903897 \r\nL 127.469265 169.453228 \r\nL 127.776392 170.452703 \r\nL 128.08352 168.598387 \r\nL 128.697776 170.699038 \r\nL 129.004904 168.285888 \r\nL 129.619159 169.476906 \r\nL 129.926287 170.182714 \r\nL 130.233415 170.183875 \r\nL 130.540542 169.59745 \r\nL 130.84767 171.657977 \r\nL 131.154798 168.980867 \r\nL 131.461926 169.27859 \r\nL 131.769054 170.494372 \r\nL 132.076181 170.371791 \r\nL 132.383309 172.659408 \r\nL 132.690437 167.803441 \r\nL 132.997565 170.028217 \r\nL 133.304693 171.199494 \r\nL 133.61182 170.819428 \r\nL 133.918948 170.148636 \r\nL 134.533204 172.60143 \r\nL 134.840331 169.611048 \r\nL 135.147459 172.339896 \r\nL 135.454587 171.978451 \r\nL 135.761715 172.103532 \r\nL 136.068843 170.531716 \r\nL 136.37597 170.977099 \r\nL 136.683098 172.169874 \r\nL 136.990226 172.246804 \r\nL 137.604482 171.448557 \r\nL 137.911609 172.762425 \r\nL 138.218737 171.515608 \r\nL 138.525865 173.092372 \r\nL 138.832993 171.045907 \r\nL 139.14012 172.558491 \r\nL 139.447248 172.485073 \r\nL 139.754376 172.102594 \r\nL 140.061504 172.949437 \r\nL 140.368632 171.852895 \r\nL 140.675759 171.684486 \r\nL 140.982887 173.951515 \r\nL 141.290015 172.391736 \r\nL 141.597143 173.753977 \r\nL 141.904271 172.863508 \r\nL 142.211398 174.424906 \r\nL 143.132782 174.23425 \r\nL 143.439909 172.137335 \r\nL 144.054165 174.202312 \r\nL 144.361293 173.828945 \r\nL 144.668421 173.163685 \r\nL 144.975548 174.162983 \r\nL 145.282676 173.357036 \r\nL 145.589804 174.329773 \r\nL 145.896932 172.968458 \r\nL 146.511187 175.145708 \r\nL 146.818315 173.821743 \r\nL 147.125443 174.724672 \r\nL 147.432571 174.54108 \r\nL 148.046826 173.585728 \r\nL 148.353954 173.338472 \r\nL 148.661082 173.782288 \r\nL 148.96821 173.356052 \r\nL 149.275337 173.639393 \r\nL 149.582465 174.760711 \r\nL 149.889593 174.603784 \r\nL 150.196721 175.812415 \r\nL 150.503849 172.27797 \r\nL 150.810976 176.979092 \r\nL 151.425232 174.727509 \r\nL 151.73236 175.685246 \r\nL 152.039487 174.887657 \r\nL 152.653743 176.275058 \r\nL 152.960871 175.871852 \r\nL 153.882254 176.362583 \r\nL 154.189382 175.329202 \r\nL 154.803638 176.777081 \r\nL 155.110765 176.352326 \r\nL 155.725021 174.482026 \r\nL 156.032149 177.373991 \r\nL 156.339276 175.690607 \r\nL 156.646404 177.066125 \r\nL 156.953532 174.996486 \r\nL 157.26066 174.470173 \r\nL 157.567788 176.027567 \r\nL 157.874915 176.493281 \r\nL 158.182043 176.118765 \r\nL 158.489171 177.56169 \r\nL 158.796299 178.045779 \r\nL 159.103427 176.050307 \r\nL 159.410554 176.887002 \r\nL 159.717682 176.275744 \r\nL 160.02481 178.636402 \r\nL 160.331938 177.598799 \r\nL 160.639066 177.701673 \r\nL 160.946193 176.397822 \r\nL 161.253321 176.42571 \r\nL 161.560449 178.674004 \r\nL 162.174704 176.748043 \r\nL 162.481832 177.804472 \r\nL 162.78896 179.639784 \r\nL 163.096088 176.846443 \r\nL 163.403216 179.596188 \r\nL 163.710343 175.069349 \r\nL 164.017471 167.736029 \r\nL 164.324599 166.990915 \r\nL 164.631727 165.93265 \r\nL 164.938855 173.684941 \r\nL 165.245982 174.996904 \r\nL 165.55311 172.690814 \r\nL 165.860238 175.361455 \r\nL 166.474493 175.538267 \r\nL 166.781621 175.03601 \r\nL 167.088749 176.429211 \r\nL 167.395877 174.802552 \r\nL 167.703005 176.14492 \r\nL 168.31726 175.776061 \r\nL 168.624388 175.285193 \r\nL 168.931516 175.700658 \r\nL 169.545771 176.822308 \r\nL 169.852899 176.589096 \r\nL 170.160027 176.930158 \r\nL 170.467155 176.853325 \r\nL 170.774282 177.83486 \r\nL 171.388538 177.687125 \r\nL 171.695666 177.047831 \r\nL 172.002794 178.002319 \r\nL 172.309921 177.886557 \r\nL 172.617049 178.468354 \r\nL 172.924177 177.652127 \r\nL 173.538433 178.455059 \r\nL 173.84556 175.669904 \r\nL 174.152688 178.137514 \r\nL 174.459816 177.642854 \r\nL 174.766944 179.056111 \r\nL 175.074071 177.863549 \r\nL 175.381199 177.277668 \r\nL 175.688327 177.660261 \r\nL 175.995455 179.31383 \r\nL 176.302583 179.295604 \r\nL 176.60971 179.836745 \r\nL 176.916838 179.283574 \r\nL 177.223966 180.013391 \r\nL 177.531094 178.742022 \r\nL 177.838222 179.817844 \r\nL 178.145349 179.628875 \r\nL 178.452477 180.649213 \r\nL 178.759605 180.047543 \r\nL 179.066733 180.051118 \r\nL 179.37386 178.913125 \r\nL 179.680988 180.963263 \r\nL 179.988116 178.874854 \r\nL 180.602372 180.452407 \r\nL 180.909499 180.308912 \r\nL 181.216627 179.789824 \r\nL 181.523755 180.562437 \r\nL 181.830883 179.377426 \r\nL 182.138011 181.570259 \r\nL 182.445138 181.657566 \r\nL 182.752266 179.008144 \r\nL 183.059394 182.059751 \r\nL 183.366522 181.789012 \r\nL 183.673649 179.333177 \r\nL 183.980777 179.499446 \r\nL 184.287905 180.149639 \r\nL 184.595033 180.143935 \r\nL 184.902161 182.731449 \r\nL 185.516416 179.533204 \r\nL 185.823544 182.888797 \r\nL 186.130672 180.511249 \r\nL 186.4378 180.641822 \r\nL 186.744927 181.227025 \r\nL 187.052055 181.265062 \r\nL 187.359183 182.299824 \r\nL 187.666311 180.639848 \r\nL 187.973439 181.490013 \r\nL 188.280566 181.403712 \r\nL 188.587694 182.785978 \r\nL 188.894822 183.109132 \r\nL 189.20195 180.972009 \r\nL 189.509077 182.782191 \r\nL 189.816205 181.992668 \r\nL 190.123333 184.007827 \r\nL 190.430461 181.786823 \r\nL 190.737589 181.253646 \r\nL 191.044716 183.001159 \r\nL 191.351844 181.992119 \r\nL 191.658972 180.401145 \r\nL 191.9661 182.901323 \r\nL 192.273228 181.1758 \r\nL 192.580355 182.863352 \r\nL 192.887483 182.787832 \r\nL 193.194611 182.862714 \r\nL 193.501739 181.063682 \r\nL 193.808866 183.432626 \r\nL 194.115994 183.521542 \r\nL 194.423122 183.842774 \r\nL 194.73025 183.785596 \r\nL 195.037378 181.794726 \r\nL 195.651633 182.547849 \r\nL 195.958761 181.592978 \r\nL 196.265889 184.553836 \r\nL 196.573017 182.245333 \r\nL 196.880144 184.298551 \r\nL 197.187272 183.201248 \r\nL 197.4944 183.559075 \r\nL 197.801528 183.001084 \r\nL 198.108655 183.017814 \r\nL 198.415783 185.160491 \r\nL 198.722911 183.790459 \r\nL 199.030039 183.989347 \r\nL 199.337167 183.177985 \r\nL 199.644294 183.925379 \r\nL 199.951422 183.599379 \r\nL 200.25855 182.956243 \r\nL 200.565678 184.845858 \r\nL 200.872806 184.042245 \r\nL 201.179933 185.783073 \r\nL 201.487061 183.769418 \r\nL 201.794189 185.314593 \r\nL 202.101317 185.076584 \r\nL 202.408444 184.695996 \r\nL 202.715572 183.307946 \r\nL 203.0227 184.463442 \r\nL 203.329828 184.883869 \r\nL 203.636956 185.638207 \r\nL 203.944083 184.2546 \r\nL 204.251211 184.795706 \r\nL 204.558339 184.315198 \r\nL 204.865467 186.167048 \r\nL 205.479722 184.643057 \r\nL 205.78685 184.377081 \r\nL 206.093978 185.931119 \r\nL 206.708233 184.865451 \r\nL 207.015361 185.645149 \r\nL 207.322489 184.391628 \r\nL 207.629617 186.159082 \r\nL 208.243872 186.022906 \r\nL 208.551 185.559117 \r\nL 208.858128 185.444456 \r\nL 209.165256 186.215862 \r\nL 209.472384 186.256802 \r\nL 209.779511 186.135657 \r\nL 210.086639 186.882084 \r\nL 210.393767 186.90962 \r\nL 210.700895 185.762785 \r\nL 211.008022 186.940259 \r\nL 211.31515 186.170023 \r\nL 211.622278 187.439782 \r\nL 211.929406 187.454167 \r\nL 212.236534 186.354206 \r\nL 212.543661 186.785677 \r\nL 212.850789 186.639047 \r\nL 213.157917 188.153373 \r\nL 213.465045 188.274653 \r\nL 213.772173 187.684553 \r\nL 214.0793 188.498028 \r\nL 214.386428 187.730924 \r\nL 214.693556 187.572666 \r\nL 215.000684 186.226056 \r\nL 215.307811 186.970078 \r\nL 215.614939 188.648617 \r\nL 215.922067 187.406497 \r\nL 216.229195 187.100768 \r\nL 216.536323 186.974637 \r\nL 216.84345 188.582464 \r\nL 217.150578 188.220098 \r\nL 217.457706 186.717144 \r\nL 217.764834 186.778975 \r\nL 218.071962 187.90824 \r\nL 218.686217 187.354294 \r\nL 218.993345 187.136399 \r\nL 219.300473 188.552534 \r\nL 219.607601 188.696041 \r\nL 219.914728 186.559076 \r\nL 220.221856 187.756701 \r\nL 220.528984 188.440725 \r\nL 220.836112 188.333372 \r\nL 221.143239 188.761831 \r\nL 221.450367 188.122024 \r\nL 221.757495 188.655307 \r\nL 222.064623 186.470821 \r\nL 222.371751 189.812205 \r\nL 222.678878 189.45054 \r\nL 222.986006 188.75456 \r\nL 223.293134 189.255507 \r\nL 223.600262 187.377679 \r\nL 223.90739 189.610448 \r\nL 224.214517 188.999404 \r\nL 224.521645 190.07746 \r\nL 224.828773 187.47023 \r\nL 225.135901 190.424635 \r\nL 225.443028 188.1931 \r\nL 225.750156 187.738495 \r\nL 226.364412 188.796598 \r\nL 226.67154 188.469651 \r\nL 226.978667 188.617185 \r\nL 227.285795 190.134483 \r\nL 227.592923 189.324243 \r\nL 227.900051 189.58389 \r\nL 228.207179 191.0109 \r\nL 228.514306 190.006396 \r\nL 228.821434 190.418245 \r\nL 229.128562 190.2922 \r\nL 229.43569 188.64767 \r\nL 230.049945 190.100374 \r\nL 230.357073 188.620729 \r\nL 230.971329 191.642963 \r\nL 231.278456 190.868388 \r\nL 231.585584 190.781704 \r\nL 231.892712 190.874515 \r\nL 232.19984 190.020126 \r\nL 232.506968 190.656588 \r\nL 232.814095 191.641173 \r\nL 233.121223 190.93267 \r\nL 233.428351 191.773639 \r\nL 233.735479 190.490851 \r\nL 234.042606 190.045579 \r\nL 234.349734 190.614945 \r\nL 234.656862 189.157736 \r\nL 234.96399 190.21402 \r\nL 235.271118 191.78663 \r\nL 235.578245 191.135025 \r\nL 236.192501 192.803041 \r\nL 236.499629 190.143917 \r\nL 236.806757 191.708281 \r\nL 237.113884 192.616976 \r\nL 237.421012 191.201004 \r\nL 237.72814 193.599386 \r\nL 238.035268 191.274574 \r\nL 238.342395 190.963487 \r\nL 238.649523 189.531509 \r\nL 238.956651 191.182252 \r\nL 239.263779 191.144679 \r\nL 239.570907 191.260755 \r\nL 239.878034 192.001797 \r\nL 240.185162 191.573209 \r\nL 240.49229 190.289383 \r\nL 240.799418 192.212864 \r\nL 241.106546 192.712513 \r\nL 241.720801 191.925152 \r\nL 242.027929 191.718186 \r\nL 242.335057 193.19518 \r\nL 242.642184 191.896077 \r\nL 242.949312 194.208713 \r\nL 243.25644 193.483404 \r\nL 243.563568 193.113052 \r\nL 243.870696 193.419562 \r\nL 244.177823 192.135862 \r\nL 244.484951 193.308233 \r\nL 244.792079 193.517107 \r\nL 245.099207 193.864276 \r\nL 245.406335 192.872175 \r\nL 245.713462 193.318367 \r\nL 246.02059 194.005995 \r\nL 246.327718 194.383235 \r\nL 246.634846 193.913367 \r\nL 246.941974 194.207398 \r\nL 247.249101 193.369707 \r\nL 247.556229 193.590019 \r\nL 247.863357 194.096406 \r\nL 248.170485 193.982532 \r\nL 248.477612 194.004173 \r\nL 248.78474 193.89779 \r\nL 249.091868 194.313337 \r\nL 249.398996 194.341059 \r\nL 249.706124 194.851191 \r\nL 250.627507 194.438684 \r\nL 250.934635 193.651109 \r\nL 251.241763 195.514884 \r\nL 251.54889 195.481298 \r\nL 251.856018 194.061888 \r\nL 252.163146 194.283476 \r\nL 252.777401 195.819229 \r\nL 253.084529 194.987015 \r\nL 253.391657 194.608878 \r\nL 253.698785 195.646009 \r\nL 254.005913 193.800708 \r\nL 254.31304 194.050876 \r\nL 254.620168 193.776991 \r\nL 254.927296 194.987696 \r\nL 255.234424 194.487593 \r\nL 255.541552 196.232142 \r\nL 255.848679 195.664309 \r\nL 256.155807 196.240056 \r\nL 256.462935 197.504106 \r\nL 256.770063 195.793068 \r\nL 257.07719 195.325389 \r\nL 257.384318 195.071615 \r\nL 257.998574 196.972894 \r\nL 258.305702 196.718896 \r\nL 258.612829 195.368116 \r\nL 258.919957 196.844423 \r\nL 259.227085 197.057802 \r\nL 259.534213 193.554231 \r\nL 259.841341 195.884721 \r\nL 260.148468 196.110399 \r\nL 260.455596 196.750114 \r\nL 260.762724 196.020568 \r\nL 261.069852 196.513987 \r\nL 261.376979 196.354797 \r\nL 261.684107 197.648875 \r\nL 261.991235 196.70793 \r\nL 262.298363 196.549049 \r\nL 262.605491 198.047769 \r\nL 262.912618 195.976474 \r\nL 263.219746 197.125208 \r\nL 263.526874 197.697468 \r\nL 263.834002 197.238859 \r\nL 264.14113 197.47304 \r\nL 264.448257 196.874322 \r\nL 264.755385 197.614943 \r\nL 265.062513 197.251891 \r\nL 265.369641 198.394504 \r\nL 265.676768 197.665459 \r\nL 265.983896 195.968279 \r\nL 266.291024 197.945219 \r\nL 266.598152 198.091821 \r\nL 266.90528 197.675063 \r\nL 267.212407 198.177824 \r\nL 267.519535 197.901262 \r\nL 267.826663 197.170346 \r\nL 268.133791 198.00046 \r\nL 268.440919 199.924264 \r\nL 268.748046 198.035364 \r\nL 269.055174 198.243683 \r\nL 269.362302 199.849153 \r\nL 269.976557 197.700268 \r\nL 270.590813 199.12769 \r\nL 270.897941 198.190198 \r\nL 271.512196 199.363202 \r\nL 271.819324 198.954965 \r\nL 272.126452 199.507098 \r\nL 272.43358 199.621587 \r\nL 273.047835 198.797191 \r\nL 273.354963 199.738733 \r\nL 273.662091 199.146208 \r\nL 274.276346 199.850108 \r\nL 274.583474 199.764113 \r\nL 274.890602 199.086253 \r\nL 275.19773 200.743669 \r\nL 275.504858 199.220747 \r\nL 275.811985 200.270278 \r\nL 276.119113 199.057544 \r\nL 276.426241 200.517788 \r\nL 276.733369 201.222169 \r\nL 277.040497 200.816689 \r\nL 277.347624 199.962403 \r\nL 277.654752 201.10864 \r\nL 277.96188 201.310995 \r\nL 278.269008 201.024702 \r\nL 278.576136 200.12923 \r\nL 278.883263 201.346103 \r\nL 279.190391 200.620982 \r\nL 279.497519 200.646622 \r\nL 279.804647 201.317231 \r\nL 280.111774 201.362732 \r\nL 280.418902 201.161828 \r\nL 280.72603 201.680735 \r\nL 281.033158 200.562732 \r\nL 281.340286 201.001193 \r\nL 281.647413 200.921571 \r\nL 281.954541 201.281148 \r\nL 282.261669 202.252538 \r\nL 282.568797 201.397159 \r\nL 282.875925 202.338035 \r\nL 283.183052 202.265478 \r\nL 283.49018 200.971143 \r\nL 283.797308 201.967978 \r\nL 284.104436 200.77821 \r\nL 284.411563 201.879025 \r\nL 284.718691 201.650464 \r\nL 285.025819 202.503987 \r\nL 285.640075 202.929076 \r\nL 285.947202 202.081148 \r\nL 286.25433 201.993017 \r\nL 286.561458 203.208393 \r\nL 286.868586 202.664172 \r\nL 287.175714 202.384666 \r\nL 287.482841 202.832818 \r\nL 287.789969 202.09779 \r\nL 288.404225 201.533647 \r\nL 288.711352 203.609753 \r\nL 289.01848 203.417252 \r\nL 289.325608 202.37463 \r\nL 289.632736 202.707858 \r\nL 289.939864 201.059495 \r\nL 290.246991 202.569908 \r\nL 290.554119 201.376525 \r\nL 290.861247 203.723433 \r\nL 291.168375 202.988273 \r\nL 291.475503 202.976394 \r\nL 292.089758 203.744433 \r\nL 292.396886 203.552642 \r\nL 292.704014 203.842768 \r\nL 293.011141 202.218002 \r\nL 293.318269 203.727049 \r\nL 293.625397 204.564974 \r\nL 293.932525 204.548098 \r\nL 294.239653 204.229234 \r\nL 294.853908 205.094147 \r\nL 295.161036 204.424195 \r\nL 295.468164 203.471403 \r\nL 295.775292 203.98979 \r\nL 296.082419 203.655178 \r\nL 296.389547 204.754952 \r\nL 297.003803 204.812132 \r\nL 297.925186 203.75852 \r\nL 298.232314 205.472244 \r\nL 298.539442 204.517544 \r\nL 298.846569 204.899008 \r\nL 299.153697 204.58505 \r\nL 299.460825 205.324249 \r\nL 299.767953 205.727868 \r\nL 300.075081 205.936723 \r\nL 300.382208 203.667906 \r\nL 300.689336 205.110076 \r\nL 300.996464 203.666284 \r\nL 301.303592 205.112086 \r\nL 301.610719 204.944662 \r\nL 301.917847 206.761498 \r\nL 302.224975 205.681455 \r\nL 302.532103 205.86305 \r\nL 302.839231 205.187084 \r\nL 303.146358 205.153978 \r\nL 303.453486 206.322184 \r\nL 303.760614 205.444714 \r\nL 304.067742 206.158959 \r\nL 304.37487 205.605925 \r\nL 304.681997 205.773049 \r\nL 304.989125 206.449158 \r\nL 305.603381 205.266511 \r\nL 305.910508 207.227681 \r\nL 306.524764 205.80379 \r\nL 306.831892 205.953902 \r\nL 307.13902 207.578319 \r\nL 307.446147 207.176406 \r\nL 307.753275 207.231981 \r\nL 308.060403 206.133184 \r\nL 308.367531 206.260424 \r\nL 308.674659 207.184981 \r\nL 308.981786 206.0854 \r\nL 309.596042 207.212167 \r\nL 309.90317 206.141466 \r\nL 310.824553 207.586137 \r\nL 311.131681 207.316798 \r\nL 311.745936 207.521497 \r\nL 312.053064 206.703824 \r\nL 312.360192 207.816209 \r\nL 312.974448 207.647274 \r\nL 313.588703 208.041868 \r\nL 313.895831 207.585024 \r\nL 314.202959 208.016997 \r\nL 314.510087 206.520396 \r\nL 314.817214 208.444206 \r\nL 315.43147 208.470435 \r\nL 315.738598 208.066078 \r\nL 316.045725 208.708573 \r\nL 316.352853 208.271276 \r\nL 316.659981 208.415681 \r\nL 316.967109 208.134741 \r\nL 317.274237 206.923419 \r\nL 317.581364 208.446581 \r\nL 317.888492 207.616153 \r\nL 318.19562 207.89354 \r\nL 318.809876 209.156462 \r\nL 319.117003 208.881147 \r\nL 319.424131 208.94927 \r\nL 319.731259 208.168843 \r\nL 320.038387 209.691724 \r\nL 320.345514 208.367783 \r\nL 320.652642 209.942235 \r\nL 320.95977 209.578286 \r\nL 321.266898 208.762134 \r\nL 321.574026 208.782532 \r\nL 321.881153 208.022108 \r\nL 322.188281 209.632016 \r\nL 322.495409 209.440704 \r\nL 322.802537 210.417975 \r\nL 323.109665 208.22859 \r\nL 323.416792 209.730898 \r\nL 324.031048 209.174387 \r\nL 324.338176 209.644354 \r\nL 324.645303 209.477874 \r\nL 324.952431 208.565335 \r\nL 325.259559 209.906048 \r\nL 325.566687 208.820055 \r\nL 325.873815 209.878692 \r\nL 326.48807 208.947746 \r\nL 326.795198 210.349263 \r\nL 327.409454 209.209998 \r\nL 327.716581 210.56518 \r\nL 328.023709 210.822929 \r\nL 328.330837 209.189966 \r\nL 328.945092 211.15996 \r\nL 329.559348 209.673275 \r\nL 329.866476 209.926256 \r\nL 330.173604 210.460829 \r\nL 330.480731 209.793319 \r\nL 330.787859 210.882529 \r\nL 331.094987 210.537563 \r\nL 331.402115 210.976828 \r\nL 331.709243 211.658383 \r\nL 332.01637 211.214543 \r\nL 332.323498 211.134472 \r\nL 332.630626 210.549288 \r\nL 332.937754 211.775188 \r\nL 333.244881 211.697222 \r\nL 333.859137 211.04978 \r\nL 334.166265 211.41888 \r\nL 334.473393 212.571649 \r\nL 334.78052 210.770506 \r\nL 335.087648 210.494542 \r\nL 335.394776 211.333104 \r\nL 335.701904 211.210088 \r\nL 336.009032 211.429097 \r\nL 336.316159 211.152158 \r\nL 336.623287 211.999744 \r\nL 336.930415 210.630698 \r\nL 337.544671 211.897852 \r\nL 337.851798 211.627266 \r\nL 338.158926 211.898204 \r\nL 338.773182 211.343441 \r\nL 339.080309 213.195446 \r\nL 339.387437 211.99721 \r\nL 339.694565 211.433802 \r\nL 340.001693 211.425158 \r\nL 340.308821 212.755962 \r\nL 340.615948 212.384087 \r\nL 340.923076 212.524947 \r\nL 341.230204 212.185353 \r\nL 341.537332 212.240323 \r\nL 341.84446 212.149153 \r\nL 342.151587 212.548916 \r\nL 342.458715 212.621953 \r\nL 342.765843 211.668859 \r\nL 343.072971 212.719599 \r\nL 343.380098 213.107171 \r\nL 343.687226 211.802341 \r\nL 343.994354 212.427845 \r\nL 344.301482 212.562208 \r\nL 344.915737 213.463367 \r\nL 345.222865 212.12179 \r\nL 345.529993 212.834204 \r\nL 345.837121 213.109378 \r\nL 346.144249 213.757588 \r\nL 346.451376 213.505963 \r\nL 346.758504 212.882785 \r\nL 347.065632 213.333072 \r\nL 347.37276 212.388963 \r\nL 347.679887 212.729167 \r\nL 347.987015 213.28024 \r\nL 348.294143 213.332564 \r\nL 348.601271 212.701297 \r\nL 348.908399 213.33592 \r\nL 349.522654 212.988183 \r\nL 349.829782 213.506668 \r\nL 350.13691 213.435119 \r\nL 350.444038 214.03974 \r\nL 350.751165 214.382814 \r\nL 351.058293 214.06961 \r\nL 351.365421 213.196107 \r\nL 351.672549 213.275787 \r\nL 351.979676 212.764579 \r\nL 352.286804 213.966759 \r\nL 352.593932 213.775448 \r\nL 352.90106 213.931672 \r\nL 353.208188 213.417384 \r\nL 353.515315 214.756364 \r\nL 353.822443 213.343123 \r\nL 353.822443 213.343123 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 34.240625 224.64 \r\nL 34.240625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 369.040625 224.64 \r\nL 369.040625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 34.240625 224.64 \r\nL 369.040625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 34.240625 7.2 \r\nL 369.040625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p9654f745d8\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"34.240625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 1000/1000 [04:14<00:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 376.511593 262.19625\" width=\"376.511593pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-31T21:03:03.529793</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 376.511593 262.19625 \r\nL 376.511593 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 34.240625 224.64 \r\nL 369.040625 224.64 \r\nL 369.040625 7.2 \r\nL 34.240625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mddc07c4e0d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.458807\" xlink:href=\"#mddc07c4e0d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(46.277557 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"110.884364\" xlink:href=\"#mddc07c4e0d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(101.340614 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"172.309921\" xlink:href=\"#mddc07c4e0d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(162.766171 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"233.735479\" xlink:href=\"#mddc07c4e0d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(224.191729 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"295.161036\" xlink:href=\"#mddc07c4e0d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(285.617286 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.586593\" xlink:href=\"#mddc07c4e0d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(343.861593 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Iterations -->\r\n     <g transform=\"translate(177.827344 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 1259 4666 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-49\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3597 1894 \r\nL 3597 1613 \r\nL 953 1613 \r\nQ 991 1019 1311 708 \r\nQ 1631 397 2203 397 \r\nQ 2534 397 2845 478 \r\nQ 3156 559 3463 722 \r\nL 3463 178 \r\nQ 3153 47 2828 -22 \r\nQ 2503 -91 2169 -91 \r\nQ 1331 -91 842 396 \r\nQ 353 884 353 1716 \r\nQ 353 2575 817 3079 \r\nQ 1281 3584 2069 3584 \r\nQ 2775 3584 3186 3129 \r\nQ 3597 2675 3597 1894 \r\nz\r\nM 3022 2063 \r\nQ 3016 2534 2758 2815 \r\nQ 2500 3097 2075 3097 \r\nQ 1594 3097 1305 2825 \r\nQ 1016 2553 972 2059 \r\nL 3022 2063 \r\nz\r\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-49\"/>\r\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"68.701172\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"130.224609\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"171.337891\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"232.617188\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"271.826172\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"299.609375\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"360.791016\" xlink:href=\"#DejaVuSans-6e\"/>\r\n      <use x=\"424.169922\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"md4702d2e46\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#md4702d2e46\" y=\"181.197047\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(20.878125 184.996265)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#md4702d2e46\" y=\"133.209158\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(20.878125 137.008377)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#md4702d2e46\" y=\"85.22127\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(20.878125 89.020489)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#md4702d2e46\" y=\"37.233382\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(20.878125 41.0326)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_12\">\r\n     <!-- Loss -->\r\n     <g transform=\"translate(14.798437 126.887187)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 1259 4666 \r\nL 1259 531 \r\nL 3531 531 \r\nL 3531 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-4c\"/>\r\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_11\">\r\n    <path clip-path=\"url(#p111c7ccd80)\" d=\"M 49.458807 17.083636 \r\nL 49.765935 18.880775 \r\nL 50.073062 42.160353 \r\nL 50.38019 33.436801 \r\nL 50.994446 52.265465 \r\nL 51.608701 64.901222 \r\nL 51.915829 66.087847 \r\nL 52.222957 62.530182 \r\nL 52.530085 65.675449 \r\nL 52.837212 66.531182 \r\nL 53.14434 66.28102 \r\nL 53.451468 66.838722 \r\nL 53.758596 67.829724 \r\nL 54.372851 66.789341 \r\nL 54.679979 66.634347 \r\nL 54.987107 68.154758 \r\nL 55.294235 67.99116 \r\nL 55.601363 69.530334 \r\nL 55.90849 67.899653 \r\nL 56.215618 68.652255 \r\nL 56.522746 68.68357 \r\nL 57.137001 71.194731 \r\nL 57.444129 71.170121 \r\nL 58.058385 72.360395 \r\nL 58.979768 73.895828 \r\nL 59.286896 74.575927 \r\nL 59.594024 79.179639 \r\nL 59.901152 77.828113 \r\nL 61.129663 85.074159 \r\nL 61.43679 86.212067 \r\nL 61.743918 86.22384 \r\nL 62.051046 87.09955 \r\nL 62.358174 90.133631 \r\nL 62.665302 89.391555 \r\nL 62.972429 91.165044 \r\nL 63.279557 92.153175 \r\nL 63.586685 94.167617 \r\nL 63.893813 94.275896 \r\nL 64.200941 96.930164 \r\nL 64.508068 98.425117 \r\nL 64.815196 98.829839 \r\nL 65.122324 99.786518 \r\nL 65.429452 101.422633 \r\nL 65.736579 101.553452 \r\nL 66.657963 108.502244 \r\nL 66.965091 104.954555 \r\nL 68.193602 114.478763 \r\nL 68.50073 114.630794 \r\nL 68.807857 118.506913 \r\nL 69.114985 116.813203 \r\nL 69.422113 119.400941 \r\nL 69.729241 120.106028 \r\nL 70.036369 121.884952 \r\nL 70.343496 122.138695 \r\nL 70.650624 126.182405 \r\nL 70.957752 126.397843 \r\nL 71.26488 128.107194 \r\nL 72.186263 129.785103 \r\nL 72.493391 132.832411 \r\nL 72.800519 131.232381 \r\nL 73.107646 134.515852 \r\nL 73.414774 133.898571 \r\nL 73.721902 134.999993 \r\nL 74.02903 137.188689 \r\nL 74.643285 136.805815 \r\nL 75.257541 138.430443 \r\nL 75.564669 140.09908 \r\nL 75.871796 139.733362 \r\nL 76.178924 143.306261 \r\nL 76.486052 141.417622 \r\nL 76.79318 140.765158 \r\nL 77.407435 145.39364 \r\nL 77.714563 145.522325 \r\nL 78.021691 146.227521 \r\nL 78.635947 143.532648 \r\nL 78.943074 145.243909 \r\nL 79.250202 144.883935 \r\nL 79.55733 148.687878 \r\nL 79.864458 147.506848 \r\nL 80.171585 145.75838 \r\nL 80.478713 148.924853 \r\nL 80.785841 148.597955 \r\nL 81.092969 148.521711 \r\nL 81.707224 150.773763 \r\nL 82.014352 150.184759 \r\nL 82.32148 150.50549 \r\nL 82.628608 151.770109 \r\nL 82.935736 150.373379 \r\nL 83.242863 152.07698 \r\nL 83.549991 150.985157 \r\nL 83.857119 153.80808 \r\nL 84.164247 152.848105 \r\nL 84.471374 152.805475 \r\nL 84.778502 152.999209 \r\nL 85.08563 152.363662 \r\nL 85.699886 154.376416 \r\nL 86.007013 154.667371 \r\nL 86.621269 158.012693 \r\nL 86.928397 154.289703 \r\nL 87.235525 152.612811 \r\nL 88.156908 156.402218 \r\nL 88.464036 153.94983 \r\nL 88.771163 155.111433 \r\nL 89.078291 154.609032 \r\nL 89.385419 156.005248 \r\nL 89.692547 155.178587 \r\nL 90.306802 155.39097 \r\nL 90.921058 158.107587 \r\nL 91.535314 155.809993 \r\nL 91.842441 155.758873 \r\nL 92.149569 158.219671 \r\nL 92.456697 157.464368 \r\nL 92.763825 155.91782 \r\nL 93.070952 157.049052 \r\nL 93.37808 159.852084 \r\nL 93.685208 157.821734 \r\nL 93.992336 158.006561 \r\nL 94.299464 158.913465 \r\nL 94.606591 155.288583 \r\nL 94.913719 159.86241 \r\nL 95.220847 157.878831 \r\nL 95.527975 158.191817 \r\nL 96.449358 159.514454 \r\nL 96.756486 159.125482 \r\nL 97.063614 159.575425 \r\nL 97.370742 159.405357 \r\nL 97.677869 161.98576 \r\nL 97.984997 160.262806 \r\nL 98.292125 159.294612 \r\nL 98.599253 159.054758 \r\nL 98.90638 160.242178 \r\nL 99.213508 159.959832 \r\nL 99.520636 161.031118 \r\nL 99.827764 158.835368 \r\nL 100.442019 162.278724 \r\nL 100.749147 159.660856 \r\nL 101.056275 160.884951 \r\nL 101.363403 159.919582 \r\nL 101.670531 161.919075 \r\nL 101.977658 162.585297 \r\nL 102.284786 162.150897 \r\nL 102.591914 163.669139 \r\nL 103.206169 161.698763 \r\nL 103.513297 162.61402 \r\nL 103.820425 162.726607 \r\nL 104.127553 161.7686 \r\nL 104.434681 159.824517 \r\nL 104.741808 162.869399 \r\nL 105.048936 164.046687 \r\nL 105.663192 160.719351 \r\nL 105.97032 162.860486 \r\nL 106.277447 162.679521 \r\nL 106.584575 164.214821 \r\nL 107.198831 163.435881 \r\nL 107.505958 164.016328 \r\nL 107.813086 163.541427 \r\nL 108.120214 162.661278 \r\nL 108.427342 163.800564 \r\nL 108.73447 164.493712 \r\nL 109.041597 163.970283 \r\nL 109.348725 163.045616 \r\nL 109.655853 164.495411 \r\nL 109.962981 165.147468 \r\nL 110.577236 164.642917 \r\nL 110.884364 165.506333 \r\nL 111.49862 164.36243 \r\nL 111.805747 164.264185 \r\nL 112.112875 165.787569 \r\nL 112.420003 165.286599 \r\nL 112.727131 166.61617 \r\nL 113.034259 165.292205 \r\nL 113.341386 166.527787 \r\nL 113.648514 166.434118 \r\nL 113.955642 166.021862 \r\nL 114.26277 168.560282 \r\nL 114.569898 165.490407 \r\nL 114.877025 166.707213 \r\nL 115.184153 165.448795 \r\nL 115.798409 167.649528 \r\nL 116.105536 166.573237 \r\nL 116.412664 166.634733 \r\nL 116.719792 166.073296 \r\nL 117.02692 168.2085 \r\nL 117.334048 166.286772 \r\nL 117.641175 167.748889 \r\nL 117.948303 166.826099 \r\nL 118.255431 166.376877 \r\nL 118.562559 165.483748 \r\nL 118.869687 166.802868 \r\nL 119.176814 165.786574 \r\nL 119.483942 166.920952 \r\nL 119.79107 165.138247 \r\nL 120.712453 167.948698 \r\nL 121.019581 168.212207 \r\nL 121.326709 169.107641 \r\nL 121.633837 166.460593 \r\nL 121.940964 168.542743 \r\nL 122.248092 168.154732 \r\nL 122.55522 168.039982 \r\nL 122.862348 166.755947 \r\nL 123.169476 169.162381 \r\nL 123.476603 169.412675 \r\nL 123.783731 166.810442 \r\nL 124.090859 168.572216 \r\nL 124.397987 168.716632 \r\nL 124.705114 168.063986 \r\nL 125.012242 169.328788 \r\nL 125.31937 168.254099 \r\nL 125.626498 167.874085 \r\nL 125.933626 168.254059 \r\nL 126.240753 168.896362 \r\nL 126.547881 170.343034 \r\nL 126.855009 169.769858 \r\nL 127.162137 169.903897 \r\nL 127.469265 169.453228 \r\nL 127.776392 170.452703 \r\nL 128.08352 168.598387 \r\nL 128.697776 170.699038 \r\nL 129.004904 168.285888 \r\nL 129.619159 169.476906 \r\nL 129.926287 170.182714 \r\nL 130.233415 170.183875 \r\nL 130.540542 169.59745 \r\nL 130.84767 171.657977 \r\nL 131.154798 168.980867 \r\nL 131.461926 169.27859 \r\nL 131.769054 170.494372 \r\nL 132.076181 170.371791 \r\nL 132.383309 172.659408 \r\nL 132.690437 167.803441 \r\nL 132.997565 170.028217 \r\nL 133.304693 171.199494 \r\nL 133.61182 170.819428 \r\nL 133.918948 170.148636 \r\nL 134.533204 172.60143 \r\nL 134.840331 169.611048 \r\nL 135.147459 172.339896 \r\nL 135.454587 171.978451 \r\nL 135.761715 172.103532 \r\nL 136.068843 170.531716 \r\nL 136.37597 170.977099 \r\nL 136.683098 172.169874 \r\nL 136.990226 172.246804 \r\nL 137.604482 171.448557 \r\nL 137.911609 172.762425 \r\nL 138.218737 171.515608 \r\nL 138.525865 173.092372 \r\nL 138.832993 171.045907 \r\nL 139.14012 172.558491 \r\nL 139.447248 172.485073 \r\nL 139.754376 172.102594 \r\nL 140.061504 172.949437 \r\nL 140.368632 171.852895 \r\nL 140.675759 171.684486 \r\nL 140.982887 173.951515 \r\nL 141.290015 172.391736 \r\nL 141.597143 173.753977 \r\nL 141.904271 172.863508 \r\nL 142.211398 174.424906 \r\nL 143.132782 174.23425 \r\nL 143.439909 172.137335 \r\nL 144.054165 174.202312 \r\nL 144.361293 173.828945 \r\nL 144.668421 173.163685 \r\nL 144.975548 174.162983 \r\nL 145.282676 173.357036 \r\nL 145.589804 174.329773 \r\nL 145.896932 172.968458 \r\nL 146.511187 175.145708 \r\nL 146.818315 173.821743 \r\nL 147.125443 174.724672 \r\nL 147.432571 174.54108 \r\nL 148.046826 173.585728 \r\nL 148.353954 173.338472 \r\nL 148.661082 173.782288 \r\nL 148.96821 173.356052 \r\nL 149.275337 173.639393 \r\nL 149.582465 174.760711 \r\nL 149.889593 174.603784 \r\nL 150.196721 175.812415 \r\nL 150.503849 172.27797 \r\nL 150.810976 176.979092 \r\nL 151.425232 174.727509 \r\nL 151.73236 175.685246 \r\nL 152.039487 174.887657 \r\nL 152.653743 176.275058 \r\nL 152.960871 175.871852 \r\nL 153.882254 176.362583 \r\nL 154.189382 175.329202 \r\nL 154.803638 176.777081 \r\nL 155.110765 176.352326 \r\nL 155.725021 174.482026 \r\nL 156.032149 177.373991 \r\nL 156.339276 175.690607 \r\nL 156.646404 177.066125 \r\nL 156.953532 174.996486 \r\nL 157.26066 174.470173 \r\nL 157.567788 176.027567 \r\nL 157.874915 176.493281 \r\nL 158.182043 176.118765 \r\nL 158.489171 177.56169 \r\nL 158.796299 178.045779 \r\nL 159.103427 176.050307 \r\nL 159.410554 176.887002 \r\nL 159.717682 176.275744 \r\nL 160.02481 178.636402 \r\nL 160.331938 177.598799 \r\nL 160.639066 177.701673 \r\nL 160.946193 176.397822 \r\nL 161.253321 176.42571 \r\nL 161.560449 178.674004 \r\nL 162.174704 176.748043 \r\nL 162.481832 177.804472 \r\nL 162.78896 179.639784 \r\nL 163.096088 176.846443 \r\nL 163.403216 179.596188 \r\nL 163.710343 175.069349 \r\nL 164.017471 167.736029 \r\nL 164.324599 166.990915 \r\nL 164.631727 165.93265 \r\nL 164.938855 173.684941 \r\nL 165.245982 174.996904 \r\nL 165.55311 172.690814 \r\nL 165.860238 175.361455 \r\nL 166.474493 175.538267 \r\nL 166.781621 175.03601 \r\nL 167.088749 176.429211 \r\nL 167.395877 174.802552 \r\nL 167.703005 176.14492 \r\nL 168.31726 175.776061 \r\nL 168.624388 175.285193 \r\nL 168.931516 175.700658 \r\nL 169.545771 176.822308 \r\nL 169.852899 176.589096 \r\nL 170.160027 176.930158 \r\nL 170.467155 176.853325 \r\nL 170.774282 177.83486 \r\nL 171.388538 177.687125 \r\nL 171.695666 177.047831 \r\nL 172.002794 178.002319 \r\nL 172.309921 177.886557 \r\nL 172.617049 178.468354 \r\nL 172.924177 177.652127 \r\nL 173.538433 178.455059 \r\nL 173.84556 175.669904 \r\nL 174.152688 178.137514 \r\nL 174.459816 177.642854 \r\nL 174.766944 179.056111 \r\nL 175.074071 177.863549 \r\nL 175.381199 177.277668 \r\nL 175.688327 177.660261 \r\nL 175.995455 179.31383 \r\nL 176.302583 179.295604 \r\nL 176.60971 179.836745 \r\nL 176.916838 179.283574 \r\nL 177.223966 180.013391 \r\nL 177.531094 178.742022 \r\nL 177.838222 179.817844 \r\nL 178.145349 179.628875 \r\nL 178.452477 180.649213 \r\nL 178.759605 180.047543 \r\nL 179.066733 180.051118 \r\nL 179.37386 178.913125 \r\nL 179.680988 180.963263 \r\nL 179.988116 178.874854 \r\nL 180.602372 180.452407 \r\nL 180.909499 180.308912 \r\nL 181.216627 179.789824 \r\nL 181.523755 180.562437 \r\nL 181.830883 179.377426 \r\nL 182.138011 181.570259 \r\nL 182.445138 181.657566 \r\nL 182.752266 179.008144 \r\nL 183.059394 182.059751 \r\nL 183.366522 181.789012 \r\nL 183.673649 179.333177 \r\nL 183.980777 179.499446 \r\nL 184.287905 180.149639 \r\nL 184.595033 180.143935 \r\nL 184.902161 182.731449 \r\nL 185.516416 179.533204 \r\nL 185.823544 182.888797 \r\nL 186.130672 180.511249 \r\nL 186.4378 180.641822 \r\nL 186.744927 181.227025 \r\nL 187.052055 181.265062 \r\nL 187.359183 182.299824 \r\nL 187.666311 180.639848 \r\nL 187.973439 181.490013 \r\nL 188.280566 181.403712 \r\nL 188.587694 182.785978 \r\nL 188.894822 183.109132 \r\nL 189.20195 180.972009 \r\nL 189.509077 182.782191 \r\nL 189.816205 181.992668 \r\nL 190.123333 184.007827 \r\nL 190.430461 181.786823 \r\nL 190.737589 181.253646 \r\nL 191.044716 183.001159 \r\nL 191.351844 181.992119 \r\nL 191.658972 180.401145 \r\nL 191.9661 182.901323 \r\nL 192.273228 181.1758 \r\nL 192.580355 182.863352 \r\nL 192.887483 182.787832 \r\nL 193.194611 182.862714 \r\nL 193.501739 181.063682 \r\nL 193.808866 183.432626 \r\nL 194.115994 183.521542 \r\nL 194.423122 183.842774 \r\nL 194.73025 183.785596 \r\nL 195.037378 181.794726 \r\nL 195.651633 182.547849 \r\nL 195.958761 181.592978 \r\nL 196.265889 184.553836 \r\nL 196.573017 182.245333 \r\nL 196.880144 184.298551 \r\nL 197.187272 183.201248 \r\nL 197.4944 183.559075 \r\nL 197.801528 183.001084 \r\nL 198.108655 183.017814 \r\nL 198.415783 185.160491 \r\nL 198.722911 183.790459 \r\nL 199.030039 183.989347 \r\nL 199.337167 183.177985 \r\nL 199.644294 183.925379 \r\nL 199.951422 183.599379 \r\nL 200.25855 182.956243 \r\nL 200.565678 184.845858 \r\nL 200.872806 184.042245 \r\nL 201.179933 185.783073 \r\nL 201.487061 183.769418 \r\nL 201.794189 185.314593 \r\nL 202.101317 185.076584 \r\nL 202.408444 184.695996 \r\nL 202.715572 183.307946 \r\nL 203.0227 184.463442 \r\nL 203.329828 184.883869 \r\nL 203.636956 185.638207 \r\nL 203.944083 184.2546 \r\nL 204.251211 184.795706 \r\nL 204.558339 184.315198 \r\nL 204.865467 186.167048 \r\nL 205.479722 184.643057 \r\nL 205.78685 184.377081 \r\nL 206.093978 185.931119 \r\nL 206.708233 184.865451 \r\nL 207.015361 185.645149 \r\nL 207.322489 184.391628 \r\nL 207.629617 186.159082 \r\nL 208.243872 186.022906 \r\nL 208.551 185.559117 \r\nL 208.858128 185.444456 \r\nL 209.165256 186.215862 \r\nL 209.472384 186.256802 \r\nL 209.779511 186.135657 \r\nL 210.086639 186.882084 \r\nL 210.393767 186.90962 \r\nL 210.700895 185.762785 \r\nL 211.008022 186.940259 \r\nL 211.31515 186.170023 \r\nL 211.622278 187.439782 \r\nL 211.929406 187.454167 \r\nL 212.236534 186.354206 \r\nL 212.543661 186.785677 \r\nL 212.850789 186.639047 \r\nL 213.157917 188.153373 \r\nL 213.465045 188.274653 \r\nL 213.772173 187.684553 \r\nL 214.0793 188.498028 \r\nL 214.386428 187.730924 \r\nL 214.693556 187.572666 \r\nL 215.000684 186.226056 \r\nL 215.307811 186.970078 \r\nL 215.614939 188.648617 \r\nL 215.922067 187.406497 \r\nL 216.229195 187.100768 \r\nL 216.536323 186.974637 \r\nL 216.84345 188.582464 \r\nL 217.150578 188.220098 \r\nL 217.457706 186.717144 \r\nL 217.764834 186.778975 \r\nL 218.071962 187.90824 \r\nL 218.686217 187.354294 \r\nL 218.993345 187.136399 \r\nL 219.300473 188.552534 \r\nL 219.607601 188.696041 \r\nL 219.914728 186.559076 \r\nL 220.221856 187.756701 \r\nL 220.528984 188.440725 \r\nL 220.836112 188.333372 \r\nL 221.143239 188.761831 \r\nL 221.450367 188.122024 \r\nL 221.757495 188.655307 \r\nL 222.064623 186.470821 \r\nL 222.371751 189.812205 \r\nL 222.678878 189.45054 \r\nL 222.986006 188.75456 \r\nL 223.293134 189.255507 \r\nL 223.600262 187.377679 \r\nL 223.90739 189.610448 \r\nL 224.214517 188.999404 \r\nL 224.521645 190.07746 \r\nL 224.828773 187.47023 \r\nL 225.135901 190.424635 \r\nL 225.443028 188.1931 \r\nL 225.750156 187.738495 \r\nL 226.364412 188.796598 \r\nL 226.67154 188.469651 \r\nL 226.978667 188.617185 \r\nL 227.285795 190.134483 \r\nL 227.592923 189.324243 \r\nL 227.900051 189.58389 \r\nL 228.207179 191.0109 \r\nL 228.514306 190.006396 \r\nL 228.821434 190.418245 \r\nL 229.128562 190.2922 \r\nL 229.43569 188.64767 \r\nL 230.049945 190.100374 \r\nL 230.357073 188.620729 \r\nL 230.971329 191.642963 \r\nL 231.278456 190.868388 \r\nL 231.585584 190.781704 \r\nL 231.892712 190.874515 \r\nL 232.19984 190.020126 \r\nL 232.506968 190.656588 \r\nL 232.814095 191.641173 \r\nL 233.121223 190.93267 \r\nL 233.428351 191.773639 \r\nL 233.735479 190.490851 \r\nL 234.042606 190.045579 \r\nL 234.349734 190.614945 \r\nL 234.656862 189.157736 \r\nL 234.96399 190.21402 \r\nL 235.271118 191.78663 \r\nL 235.578245 191.135025 \r\nL 236.192501 192.803041 \r\nL 236.499629 190.143917 \r\nL 236.806757 191.708281 \r\nL 237.113884 192.616976 \r\nL 237.421012 191.201004 \r\nL 237.72814 193.599386 \r\nL 238.035268 191.274574 \r\nL 238.342395 190.963487 \r\nL 238.649523 189.531509 \r\nL 238.956651 191.182252 \r\nL 239.263779 191.144679 \r\nL 239.570907 191.260755 \r\nL 239.878034 192.001797 \r\nL 240.185162 191.573209 \r\nL 240.49229 190.289383 \r\nL 240.799418 192.212864 \r\nL 241.106546 192.712513 \r\nL 241.720801 191.925152 \r\nL 242.027929 191.718186 \r\nL 242.335057 193.19518 \r\nL 242.642184 191.896077 \r\nL 242.949312 194.208713 \r\nL 243.25644 193.483404 \r\nL 243.563568 193.113052 \r\nL 243.870696 193.419562 \r\nL 244.177823 192.135862 \r\nL 244.484951 193.308233 \r\nL 244.792079 193.517107 \r\nL 245.099207 193.864276 \r\nL 245.406335 192.872175 \r\nL 245.713462 193.318367 \r\nL 246.02059 194.005995 \r\nL 246.327718 194.383235 \r\nL 246.634846 193.913367 \r\nL 246.941974 194.207398 \r\nL 247.249101 193.369707 \r\nL 247.556229 193.590019 \r\nL 247.863357 194.096406 \r\nL 248.170485 193.982532 \r\nL 248.477612 194.004173 \r\nL 248.78474 193.89779 \r\nL 249.091868 194.313337 \r\nL 249.398996 194.341059 \r\nL 249.706124 194.851191 \r\nL 250.627507 194.438684 \r\nL 250.934635 193.651109 \r\nL 251.241763 195.514884 \r\nL 251.54889 195.481298 \r\nL 251.856018 194.061888 \r\nL 252.163146 194.283476 \r\nL 252.777401 195.819229 \r\nL 253.084529 194.987015 \r\nL 253.391657 194.608878 \r\nL 253.698785 195.646009 \r\nL 254.005913 193.800708 \r\nL 254.31304 194.050876 \r\nL 254.620168 193.776991 \r\nL 254.927296 194.987696 \r\nL 255.234424 194.487593 \r\nL 255.541552 196.232142 \r\nL 255.848679 195.664309 \r\nL 256.155807 196.240056 \r\nL 256.462935 197.504106 \r\nL 256.770063 195.793068 \r\nL 257.07719 195.325389 \r\nL 257.384318 195.071615 \r\nL 257.998574 196.972894 \r\nL 258.305702 196.718896 \r\nL 258.612829 195.368116 \r\nL 258.919957 196.844423 \r\nL 259.227085 197.057802 \r\nL 259.534213 193.554231 \r\nL 259.841341 195.884721 \r\nL 260.148468 196.110399 \r\nL 260.455596 196.750114 \r\nL 260.762724 196.020568 \r\nL 261.069852 196.513987 \r\nL 261.376979 196.354797 \r\nL 261.684107 197.648875 \r\nL 261.991235 196.70793 \r\nL 262.298363 196.549049 \r\nL 262.605491 198.047769 \r\nL 262.912618 195.976474 \r\nL 263.219746 197.125208 \r\nL 263.526874 197.697468 \r\nL 263.834002 197.238859 \r\nL 264.14113 197.47304 \r\nL 264.448257 196.874322 \r\nL 264.755385 197.614943 \r\nL 265.062513 197.251891 \r\nL 265.369641 198.394504 \r\nL 265.676768 197.665459 \r\nL 265.983896 195.968279 \r\nL 266.291024 197.945219 \r\nL 266.598152 198.091821 \r\nL 266.90528 197.675063 \r\nL 267.212407 198.177824 \r\nL 267.519535 197.901262 \r\nL 267.826663 197.170346 \r\nL 268.133791 198.00046 \r\nL 268.440919 199.924264 \r\nL 268.748046 198.035364 \r\nL 269.055174 198.243683 \r\nL 269.362302 199.849153 \r\nL 269.976557 197.700268 \r\nL 270.590813 199.12769 \r\nL 270.897941 198.190198 \r\nL 271.512196 199.363202 \r\nL 271.819324 198.954965 \r\nL 272.126452 199.507098 \r\nL 272.43358 199.621587 \r\nL 273.047835 198.797191 \r\nL 273.354963 199.738733 \r\nL 273.662091 199.146208 \r\nL 274.276346 199.850108 \r\nL 274.583474 199.764113 \r\nL 274.890602 199.086253 \r\nL 275.19773 200.743669 \r\nL 275.504858 199.220747 \r\nL 275.811985 200.270278 \r\nL 276.119113 199.057544 \r\nL 276.426241 200.517788 \r\nL 276.733369 201.222169 \r\nL 277.040497 200.816689 \r\nL 277.347624 199.962403 \r\nL 277.654752 201.10864 \r\nL 277.96188 201.310995 \r\nL 278.269008 201.024702 \r\nL 278.576136 200.12923 \r\nL 278.883263 201.346103 \r\nL 279.190391 200.620982 \r\nL 279.497519 200.646622 \r\nL 279.804647 201.317231 \r\nL 280.111774 201.362732 \r\nL 280.418902 201.161828 \r\nL 280.72603 201.680735 \r\nL 281.033158 200.562732 \r\nL 281.340286 201.001193 \r\nL 281.647413 200.921571 \r\nL 281.954541 201.281148 \r\nL 282.261669 202.252538 \r\nL 282.568797 201.397159 \r\nL 282.875925 202.338035 \r\nL 283.183052 202.265478 \r\nL 283.49018 200.971143 \r\nL 283.797308 201.967978 \r\nL 284.104436 200.77821 \r\nL 284.411563 201.879025 \r\nL 284.718691 201.650464 \r\nL 285.025819 202.503987 \r\nL 285.640075 202.929076 \r\nL 285.947202 202.081148 \r\nL 286.25433 201.993017 \r\nL 286.561458 203.208393 \r\nL 286.868586 202.664172 \r\nL 287.175714 202.384666 \r\nL 287.482841 202.832818 \r\nL 287.789969 202.09779 \r\nL 288.404225 201.533647 \r\nL 288.711352 203.609753 \r\nL 289.01848 203.417252 \r\nL 289.325608 202.37463 \r\nL 289.632736 202.707858 \r\nL 289.939864 201.059495 \r\nL 290.246991 202.569908 \r\nL 290.554119 201.376525 \r\nL 290.861247 203.723433 \r\nL 291.168375 202.988273 \r\nL 291.475503 202.976394 \r\nL 292.089758 203.744433 \r\nL 292.396886 203.552642 \r\nL 292.704014 203.842768 \r\nL 293.011141 202.218002 \r\nL 293.318269 203.727049 \r\nL 293.625397 204.564974 \r\nL 293.932525 204.548098 \r\nL 294.239653 204.229234 \r\nL 294.853908 205.094147 \r\nL 295.161036 204.424195 \r\nL 295.468164 203.471403 \r\nL 295.775292 203.98979 \r\nL 296.082419 203.655178 \r\nL 296.389547 204.754952 \r\nL 297.003803 204.812132 \r\nL 297.925186 203.75852 \r\nL 298.232314 205.472244 \r\nL 298.539442 204.517544 \r\nL 298.846569 204.899008 \r\nL 299.153697 204.58505 \r\nL 299.460825 205.324249 \r\nL 299.767953 205.727868 \r\nL 300.075081 205.936723 \r\nL 300.382208 203.667906 \r\nL 300.689336 205.110076 \r\nL 300.996464 203.666284 \r\nL 301.303592 205.112086 \r\nL 301.610719 204.944662 \r\nL 301.917847 206.761498 \r\nL 302.224975 205.681455 \r\nL 302.532103 205.86305 \r\nL 302.839231 205.187084 \r\nL 303.146358 205.153978 \r\nL 303.453486 206.322184 \r\nL 303.760614 205.444714 \r\nL 304.067742 206.158959 \r\nL 304.37487 205.605925 \r\nL 304.681997 205.773049 \r\nL 304.989125 206.449158 \r\nL 305.603381 205.266511 \r\nL 305.910508 207.227681 \r\nL 306.524764 205.80379 \r\nL 306.831892 205.953902 \r\nL 307.13902 207.578319 \r\nL 307.446147 207.176406 \r\nL 307.753275 207.231981 \r\nL 308.060403 206.133184 \r\nL 308.367531 206.260424 \r\nL 308.674659 207.184981 \r\nL 308.981786 206.0854 \r\nL 309.596042 207.212167 \r\nL 309.90317 206.141466 \r\nL 310.824553 207.586137 \r\nL 311.131681 207.316798 \r\nL 311.745936 207.521497 \r\nL 312.053064 206.703824 \r\nL 312.360192 207.816209 \r\nL 312.974448 207.647274 \r\nL 313.588703 208.041868 \r\nL 313.895831 207.585024 \r\nL 314.202959 208.016997 \r\nL 314.510087 206.520396 \r\nL 314.817214 208.444206 \r\nL 315.43147 208.470435 \r\nL 315.738598 208.066078 \r\nL 316.045725 208.708573 \r\nL 316.352853 208.271276 \r\nL 316.659981 208.415681 \r\nL 316.967109 208.134741 \r\nL 317.274237 206.923419 \r\nL 317.581364 208.446581 \r\nL 317.888492 207.616153 \r\nL 318.19562 207.89354 \r\nL 318.809876 209.156462 \r\nL 319.117003 208.881147 \r\nL 319.424131 208.94927 \r\nL 319.731259 208.168843 \r\nL 320.038387 209.691724 \r\nL 320.345514 208.367783 \r\nL 320.652642 209.942235 \r\nL 320.95977 209.578286 \r\nL 321.266898 208.762134 \r\nL 321.574026 208.782532 \r\nL 321.881153 208.022108 \r\nL 322.188281 209.632016 \r\nL 322.495409 209.440704 \r\nL 322.802537 210.417975 \r\nL 323.109665 208.22859 \r\nL 323.416792 209.730898 \r\nL 324.031048 209.174387 \r\nL 324.338176 209.644354 \r\nL 324.645303 209.477874 \r\nL 324.952431 208.565335 \r\nL 325.259559 209.906048 \r\nL 325.566687 208.820055 \r\nL 325.873815 209.878692 \r\nL 326.48807 208.947746 \r\nL 326.795198 210.349263 \r\nL 327.409454 209.209998 \r\nL 327.716581 210.56518 \r\nL 328.023709 210.822929 \r\nL 328.330837 209.189966 \r\nL 328.945092 211.15996 \r\nL 329.559348 209.673275 \r\nL 329.866476 209.926256 \r\nL 330.173604 210.460829 \r\nL 330.480731 209.793319 \r\nL 330.787859 210.882529 \r\nL 331.094987 210.537563 \r\nL 331.402115 210.976828 \r\nL 331.709243 211.658383 \r\nL 332.01637 211.214543 \r\nL 332.323498 211.134472 \r\nL 332.630626 210.549288 \r\nL 332.937754 211.775188 \r\nL 333.244881 211.697222 \r\nL 333.859137 211.04978 \r\nL 334.166265 211.41888 \r\nL 334.473393 212.571649 \r\nL 334.78052 210.770506 \r\nL 335.087648 210.494542 \r\nL 335.394776 211.333104 \r\nL 335.701904 211.210088 \r\nL 336.009032 211.429097 \r\nL 336.316159 211.152158 \r\nL 336.623287 211.999744 \r\nL 336.930415 210.630698 \r\nL 337.544671 211.897852 \r\nL 337.851798 211.627266 \r\nL 338.158926 211.898204 \r\nL 338.773182 211.343441 \r\nL 339.080309 213.195446 \r\nL 339.387437 211.99721 \r\nL 339.694565 211.433802 \r\nL 340.001693 211.425158 \r\nL 340.308821 212.755962 \r\nL 340.615948 212.384087 \r\nL 340.923076 212.524947 \r\nL 341.230204 212.185353 \r\nL 341.537332 212.240323 \r\nL 341.84446 212.149153 \r\nL 342.151587 212.548916 \r\nL 342.458715 212.621953 \r\nL 342.765843 211.668859 \r\nL 343.072971 212.719599 \r\nL 343.380098 213.107171 \r\nL 343.687226 211.802341 \r\nL 343.994354 212.427845 \r\nL 344.301482 212.562208 \r\nL 344.915737 213.463367 \r\nL 345.222865 212.12179 \r\nL 345.529993 212.834204 \r\nL 345.837121 213.109378 \r\nL 346.144249 213.757588 \r\nL 346.451376 213.505963 \r\nL 346.758504 212.882785 \r\nL 347.065632 213.333072 \r\nL 347.37276 212.388963 \r\nL 347.679887 212.729167 \r\nL 347.987015 213.28024 \r\nL 348.294143 213.332564 \r\nL 348.601271 212.701297 \r\nL 348.908399 213.33592 \r\nL 349.522654 212.988183 \r\nL 349.829782 213.506668 \r\nL 350.13691 213.435119 \r\nL 350.444038 214.03974 \r\nL 350.751165 214.382814 \r\nL 351.058293 214.06961 \r\nL 351.365421 213.196107 \r\nL 351.672549 213.275787 \r\nL 351.979676 212.764579 \r\nL 352.286804 213.966759 \r\nL 352.593932 213.775448 \r\nL 352.90106 213.931672 \r\nL 353.208188 213.417384 \r\nL 353.515315 214.756364 \r\nL 353.822443 213.343123 \r\nL 353.822443 213.343123 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 34.240625 224.64 \r\nL 34.240625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 369.040625 224.64 \r\nL 369.040625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 34.240625 224.64 \r\nL 369.040625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 34.240625 7.2 \r\nL 369.040625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p111c7ccd80\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"34.240625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiM0lEQVR4nO3deXwddb3/8dfnZN+apU3TvWlLFwrd6AKFAmUpVBZZ9KfCFRCQ5aosVi8XUC+IiuhVvNefqIAIioALIpRaQcCyFCilpfu+723SLUuTZjvf+8eZpGmbpkmak8mZ834+HufRM3PmzHwm08c733xn5jvmnENERIIn5HcBIiISHQp4EZGAUsCLiASUAl5EJKAU8CIiAZXodwGNdevWzRUWFvpdhohIzJg/f/5u51x+U591qoAvLCxk3rx5fpchIhIzzGzTsT5TF42ISEAp4EVEAkoBLyISUAp4EZGAUsCLiASUAl5EJKAU8CIiARXzAV9TF+axWWt5d3Wx36WIiHQqMR/wiSHjyffW84+lO/0uRUSkU4n5gDczhhRksXpXmd+liIh0KjEf8AA9uqSyu7zK7zJERDqVQAR8YsioC+vRgyIijQUi4EMhI6yAFxE5TCACPjFk1Onh4SIihwlEwIfURSMicpRABHyCKeBFRI4UjIAPGbUKeBGRwwQm4HWSVUTkcIEJeJ1kFRE5XHACXi14EZHDBCPgdZJVROQogQj4UMgIO3DqphERaRCIgE8MGYBa8SIijQQi4BO8gNelkiIihwQi4GvrIsH+8MwVPlciItJ5RD3gzSzBzBaY2YxobeNgbR0AL8zdHK1NiIjEnI5owd8FRLVpXd/3rj54EZFDohrwZtYHuBT4TTS3U10bBkD5LiJySLRb8P8D3AOEj7WAmd1qZvPMbF5xcdsenF0bPubqRUTiVtQC3swuA4qcc/ObW84594Rzbpxzblx+fn6btlVTq6a7iMiRotmCPwv4tJltBP4InG9mf4jGhmrq1IIXETlS1ALeOXefc66Pc64Q+ALwL+fcF6OxrRp1vouIHCUQ18HX1KoFLyJypA4JeOfc2865y6K1/ivH9Aagd05atDYhIhJzEv0uoD1MPbUH5w7JZ39ljd+liIh0GoHoogFISQxRVVPndxkiIp1GcAI+KYEq9cWLiDQITMCnqgUvInKYwAR8SlKIg2rBi4g0CE7AJyaoBS8i0khgAj5VLXgRkcMEJuBTEhOoCztqNWyBiAgQqICP7IqupBERiQhMwKcmJQBQqX54EREgQAGfnuwFfLUCXkQEAhTwmSmRURfKq2p9rkREpHMITMCnewFfUa2AFxGBAAV8Zkqki6a8Sl00IiIQoIBPT/Za8OqiEREBAhTw6oMXETlcYAK+/iqaCl1FIyICBCjgM7wW/AGdZBURAQIU8CmJIRJCxgF10YiIAAEKeDOjLux4bNY6Zq/Z7Xc5IiK+C0zAN/bSgq1+lyAi4rtABnw47PwuQUTEd4EM+E8278c5hbyIxLdABfy3Lz0ZgM17Kxhw30wenL7M54pERPwTqID/8tkD+eDe8xumn/lgI/e9tESteRGJS4EKeIBeOWn8/2vGMCg/A4AX5m7mV++s87kqEZGOF7iAB7h8VC/e+sbkhunXl+70rxgREZ8EMuDrzbhjEpeO7MmirSWsKy73uxwRkQ4V6IA/tXc2N501AEAnXEUk7gQ64AHG9s/lzgsG896a3XywVne4ikj8CHzAA3xl8iD65qXxX9OXUVsX9rscEZEOERcBn5qUwH2fOpm1ReW8s7rY73JERDpEXAQ8wIQBeQBs2VvhcyUiIh0jbgI+Oy0JgJJKDScsIvEhbgI+KSGyqz97c7XPlYiIdIy4CfjGdpdX+V2CiEjUxWXAr95V5ncJIiJRF1cBP+e+CwBYs0t3tYpI8MVVwBd0SaFLaiKr1IIXkTgQtYA3s1Qzm2tmi8xsmZl9N1rbakVNDCnIYo0CXkTiQDRb8FXA+c65UcBoYKqZnRHF7bXIkB5ZrN5VrjHiRSTwohbwLqK+szvJe/meqkO6Z1JSWUNRma6kEZFgi2ofvJklmNlCoAh4wzn3URPL3Gpm88xsXnFx9IcRGN4rG4CXPtkW9W2JiPgpqgHvnKtzzo0G+gATzOzUJpZ5wjk3zjk3Lj8/P5rlADC6bw6JIeM3762nLuz7HxQiIlHTIVfROOf2A7OAqR2xveYkJ4Z44PLh7DlQzXo9BEREAiyaV9Hkm1mO9z4NmAKsjNb2WmNwQRYAG3Yf8LkSEZHoiWYLvicwy8wWAx8T6YOfEcXttVhBl1QAbn12vs+ViIhET2K0VuycWwyMidb6T0TP7NSG9845zMzHakREoiOu7mStl5qUwPeujJzvXa9uGhEJqLgMeIBzBncDYM76PT5XIiISHXEb8H1z00lNCjF3w16/SxERiYq4DfhQyBjaowszl+zQ9fAiEkhxG/AAV43uRU2do1jDFohIAMV1wPfrmg7Atv2VPlciItL+4jrgC7tmALC2SMMHi0jwxH3AZ6clMXPJTr9LERFpd3Ed8KGQccOZhbyzupidJQf9LkdEpF3FdcADXDCsOwDzN+3zuRIRkfYV9wE/tEcWyQkh/jBnk9+liIi0q7gP+NSkBC46pYDlO0r9LkVEpF3FfcADjOqTQ0llDSUVNX6XIiLSbhTwQN+8yPXw33p5iR7GLSKBoYAH+nkBP2PxDrbrahoRCQgFPNA3L63h/Xbd1SoiAaGAB7JSkxreb9lb4WMlIiLtp0UBb2YZZhby3g8xs0+bWdLxvheLtuxVC15EgqGlLfh3gVQz6w38E7gOeCZaRfnhzWnnArBln1rwIhIMLQ14c85VAFcDv3TO/T/glOiV1fFO6p7JuP656qIRkcBoccCb2UTg34C/e/MSolOSf/rmpbN1n7poRCQYWhrwdwP3AX9zzi0zs4HArKhV5ZO+uWnsKKmkpi7sdykiIicssSULOefeAd4B8E627nbO3RnNwvzQJy+dsItcKtnfGyteRCRWtfQqmufNrIuZZQBLgeVm9h/RLa3jDcqPhPqMxTt8rkRE5MS1tItmuHOuFLgS+AcwgMiVNIFyWr9cMlMSWbNLT3gSkdjX0oBP8q57vxKY7pyrAQI3aIuZkZmSyMsLt1N2UAOPiUhsa2nAPw5sBDKAd82sPxDI8XV3lkbGonlj+S6fKxEROTEtPcn6c+DnjWZtMrPzolNS56AraUQk1rX0JGu2mT1qZvO810+JtOYD5717Ir+3VuxQP7yIxLaWdtH8FigDPue9SoGno1WUn/rmpXPhyQX8a2WR36WIiJyQlgb8IOfcA8659d7ru8DAaBbmp5F9stm8t4JdpRobXkRiV0sDvtLMJtVPmNlZQGDv6Z88NB+Av+t6eBGJYS06yQrcDvzezLK96X3ADdEpyX8jemeTEDL2HKjyuxQRkTZr6VU0i4BRZtbFmy41s7uBxVGszTdmRm56EnsP6Fp4EYldrXqik3Ou1LujFWBaFOrpNHLTk9UHLyIx7UQe2WftVkUnVNgtg/fWFFNeVet3KSIibXIiAR+4oQoa+8xpfaipc6zaqevhRSQ2NdsHb2ZlNB3kBqRFpaJO4rR+OSSEjJcXbGNs/1y/yxERabVmW/DOuSznXJcmXlnOuZZegROTundJ5doJ/Xh+7mY9xk9EYtKJdNE0y8z6mtksM1tuZsvM7K5obStavjChL3Vhx98WbPO7FBGRVotawAO1wDecc8OBM4CvmtnwKG6v3fXMjvRCPfrGap8rERFpvagFvHNuh3PuE+99GbAC6B2t7UVDbnpSw/twONDnlEUkgKLZgm9gZoXAGOCjJj67tX6UyuLi4o4op8XMjLyMZADeWd25ahMROZ6oB7yZZQJ/Be5udJNUA+fcE865cc65cfn5+dEup9Xuv+RkAL7xl0U+VyIi0jpRDXjvMX9/BZ5zzr0UzW1Fy6dH9SI5IUR6coLfpYiItEo0r6Ix4ClghXPu0WhtJ9qSE0Pccs4Atu6r5KnZG/wuR0SkxaLZgj8LuA4438wWeq9Lori9qBlXmAfA92Ysp7K6zudqRERaJppX0cx2zplzbqRzbrT3mhmt7UXTeUO78/yXTwfg7VV60pOIxIYOuYomCMYWRoYr+PfnPvG5EhGRllHAt1BK4qGTrA9OX+ZjJSIiLaOAb4XvXXEKAM98sBHndOOTiHRuCvg22l1e7XcJIiLNUsC3wqDumQ3vx//gTW58eq6P1YiINE8B3wpnDurGc97VNACzVhWzo6TSx4pERI5NAd9KRz78Y+IP/0VxWZVP1YiIHJsCvpVSkxK484LBh83787wtPlUjInJsCvg2mDZlCEu/e3HDdEqifowi0vkomdooM+XQEwu///cVbN6jx/qJSOeigD8Bv/y30xpa799+ZSlFZQd9rkhE5BAF/Am4ZERPVn3/UwC8u7qYCT94iw/W7fa5KhGRCAV8O/jFtWMa3l/75Eds2avuGhHxnwK+HVw2shf3fmpYw/TZP57Fv1buYuSDrzN/0z4fKxOReKaAbye3nzuILqmHTrx+5+VllB6s5TO/+oAlW0t8rExE4pUCvh09ef047r5wMBMG5LFt/6E7XC//xWxe+mTrYQOU7Sw5yN4DGs9GRKLHOtOoiOPGjXPz5s3zu4wTNmPxdr72/IKj5udnpZCcEOKr553E/X9bQlKCseYHMfmQKxHpJMxsvnNuXFOfJTY1U07MmH65Tc6vH9Lg/r8tAaCmrvP8chWR4FEXTRT0yk5teP/q1yax4YfHbqWvLSrriJJEJA6pBR8FZsbVp/Wmd04aI/pkA/D9K0/l2y8vPWrZCx99l59fM4bkBGNtUTnVdY5pU4Z0dMkiEkDqg+9gG3cfYPJP3j7ucm9OO5eTumfyvRnLufiUHkwYkBf94kQk5jTXB68umg7WLy+d687oz4OXD+dLZxby+HVjm1zuvpcWs/dANU/N3sDnHv+Qx2at7eBKRSTWqQXfCRTe+3cA8jKSm710cuMjl3ZUSSISI9SC7+TuvGAwt5w9gLn3X9Dscr//cCMX/+xdHn1jNUWlGthMRJqnFnwnU1JRQ3VdmPE/eLPZ5aYML+C/LhtOTV2YgfmZzS4rIsGl6+BjSHZ6EhC5KzYnPYnaOsc9f11EghkbG405/8byXbyxfBcAL33lTEb3yaG8upYuqZHv19SF2bqvkj3lVYwr1AlakXikFnyMqKqtI8GMrfsq+c3s9fxhzuYml5s2ZQjZaUk8MH1Zw7yl372YlMQQSQmdv0fulYXbyEpN5PxhBX6XIhITmmvBK+BjVFHpQT7asJc7Xjh6SIRjyU1P4ktnDuCuCwcff2Ef1IUdg+6fCeiEskhL6SRrAHXvksrlo3ox/Wtn0S0zpUXf2VdRw8/eXM2NT8/lTx9H/gKoqq1j5c7ShmX+9PFmnn5/Q1RqPp731hT7sl2RoFIffIwb2SeHn18zmmuf/IiJA7uSnBjindXFPHTFKVw/sZCHXl3Ob48I7Fmripm1qpjasONbf4vcXfv0l8Yzf9M+fuFdb3/jWQM6fF827D4AHD7Ug4i0nQI+ACYO7Movrh3DhScXkJIYorouTEpiAgBfOW8Qv31/A2P75x718JH6cAe48ZmPD/vs6l++z76KGt6adi6hkEV/J4DKmjogMtSDiJw4BXwAmBmXjezVMF0f7gDdMlMa+rPXFZfTLTOF0soazv7xrGbX+cnm/QAMvH8mf7z1DLqkJrF5bwVTT+3R/jvgqayOBHxduPOcFxKJZQr4ODLIu14+Oy2Jj+6/gIrqOmatLKJfXjpf/v2xT25/4Yk5De+7ZSYzZXgBD181gtlrdzO0Rxbds47uUtm8p4Ls9CSy05JaXF+FF/D1LXkROTEK+DhV0CUSygMmRfra3/j6Ocxeu5vvvrqck7pnsraovMnv7S6v5oW5W3hh7paGefdMHUpqYgI3TRrA+uJykhJCnPPfsxjcPZM3pp3b4prqg72ksoanZm/g5kkdfx5AJEgU8ALA4IIsBhdkkZGcyKTB3VhTVE51bZjzhubz4fo9XPfU3GN+98evrQLg4ZkrqG3UvbLmiF8StXVhzIyaujCpSQkcqb6LBuB7M5Yr4EVOkK6Dl1aZv2kvn/nVhy1evmd2KrO+OZmt+yqZ9ueFLPYeQP7MjeOZPLT7Ycve9uw8Xl+2q2E6Jz2JH1w5gomDupKXkdw+OyASMLrRSdqVc44P1++hd04avXLSKKmsYdz3I2PnzP7P8/j843MOe+j4sdx27kA+P64viaEQffPSuP63c9m8t4JNjYZkAMhITmD+d6Y0tPqfnbOJmYt38MKtZ7T/zonEGAW8dKjSgzWMfPCfbfruGQPz+M0N47nqsfeP6uK54/yTOH1AV7741EcAzLzzbAbmZzTZ3SMSLxTw0uFenL+VWauK+MlnR1FRXcu/Vhbx0KvLKauqpbBr+mEDpzVmBht+eCnhsGPBlv2M7Z/b5M1a9c4bms/TN06I5q6IdGq+BLyZ/Ra4DChyzp3aku8o4IOtLuzYsPsAJ3XP5JF/rOTX76w7apmM5ASWPTT1qPl/X7yDB6YvZXf50Q9EGds/l8+P78uslUWUHazl8evGMnfDXvYeqOacIfnkZ7VsKAeRWORXwJ8DlAO/V8BLU/YeqGbmkh10zUjm35/7BIARvbN59Y5Jx/zOml1lrN99gLdXFfPC3KZH1GzK9RP789AVh/83rKiuxTDSktXFI7HLty4aMysEZijg5XiWby/lmQ828M2LhtK9S8vGoqmtCzPtz4uYvmh7i7dz9uBujOmXy5Wje3H+T98B4NdfPI3BBVkNN4KJxJJOHfBmditwK0C/fv3Gbtq0KWr1SHC9OH8rs9cUU+dg34FqZq/d3eZ1fXjf+fTMTmvH6kSip1MHfGNqwUt7OFhTxx0vLGh44lVb3HbuQEb2zmHepr189byTCJlRWxdu8V8XIh1FAS9xa9XOMh6cvoxRfXO4aVIhv5y1jmc+2Njm9T15/ThG980hOSEyamfXjOQOG21TpCkKeJFG1uwq45bfz+PPt02ke5dUikoP8rM3V1N2sJYZi3e0al3/cfFQbjizkMwUjfoh/vDrKpoXgMlAN2AX8IBz7qnmvqOAFz8555i+aDtj++cy6UezuPvCwTz9/kZKKmua/V5SgnH24Hz2VVTz8FUjGNYji6raMEu3lRzzgefVtWGSE/VANTlxutFJpA2ccxyoruOhV5dxx/mD2bjnAE++t4F3VxczMD+D9cUHmvxeflYKV4zqxW9mb2DqKT1YsGUfY/vnUlFdx7uri3nkMyO558XF/OOus+mblx4zD0SXzkkBL9KOwmFHKGQ45wg7+GjDHq598qMTWuc1E/rx4KeHH/awFpGWUMCLdIC1ReV8uH4P33l56fEXPoa591/ALc/OZ9GW/fz6i2MbnqC1cmcpITOGFGS1V7kSEAp4kQ60fX8li7eWkJacwO6yKjJSEpl6ag/+MGcTRaUHeX7uFnaXVx13PZkpiXzlvEFccmpPJv/kbQD+cvtEeuek8ZXnPiEzJZGHrxpBTkYSXVJb/uQsCRYFvEgn4pzjwenLGJifyQPTlwHwyNUjuPelJW1a3/jCXP5y+5ntWaLEEAW8SCflnGPhlv2M6ZfLrtKDXPGL97ln6lBeW7qTf7byRq0ff3YkKYkhLh3Rk0SdtI0bCniRGLOz5CBvrdzF6L453P3Hhfzw6hF89teHP0mruVb/n2+bSL+8dLpnpehGrIBTwIsEQElFDQ9MX8rLCyODq2185FKKy6r45dtrefr9jU1+JyFk3HL2QIYUZPLwzJX859ShjOyTw82/+5ibzhrATY2ee3v6w28yeUh3fvTZkR2xO9JOFPAiAbJ8eynlVbVMGHDoJqon313PD2au4IHLh/PYrHUtOokL8PyXT2frvkpO65/LhY9GRtfc+MilUalbokMBLxJwzjmqasMNjy9cW1RGn9zITVSPzVrLzCU7Wb6jtEXrmjZlCBef0oMP1+3mS2cNYOu+CmYs3sFt5wzETN09nY0CXiTO1Q/DUFxWxftrd3PzpIENz7ZtTq/sVLaXHATg+VtOJzkhdMzhF8QfCngROcq+A9WM+d4bDdNJCcYFwwp4bdnOZr936YierCkq4/5LTmby0O7RLlOOQwEvIk2av2kv/fIyDntu7Zz1e7jld/Mwg2E9ujB3495jfn9Un2z2V9YwbcoQlu8oJSUxgTvPP0mXaXYgBbyItNnKnaUM6JZBSUUNEx5+67jLDy3I4vbJAxnRO4eTumfinFPffRQ1F/AaxFpEmjWsRxcAundJ4OoxvZmzfg9PXD+O7LQkpi/azjuri5m74VArf9WuMr7+p0UAZKUkUlZVy9cvHML+ympeXbSDN6edQ056MgB1YUdxWRWvLd3B58f30wPQ25la8CJywiqr6/jx6ytZtGU/n2zef9zlvzJ5EIVdM/j2K0uprg0DUNg1ndfuPocl20oYrxO5LaYuGhHpELV1YWrqHNtLKgmZMXPJDsJhx0/fWN2i75/SqwvLtpcy65uT6ZmdSnJCSHfiHocCXkR8tbPkIOVVNfTITuPB6ct4cf7WZpf/3Lg+/HleZJkHLh/OnPV7+MW1pzH4W/8AYMVDU9Wd41HAi0inUl0bJmSwcmcZO0sO0iM7lVcWbmPR1pLD+vOP5ZoJ/RjZJ5vLR/Vi34FqUpJCdM9K7YDKOx8FvIjEjLkb9nLfS4vJSk1idN8c3l1dzPrdTT8esbFpU4aQkhiiorqOUX2zOX9YQQdU6z8FvIjELOccs9fuprSylhfnb2HZ9lKKyo4/1s5VY3qzrricZ28+nfHff5PPje/Df112Cj/55ypunjSAgi7BaPEr4EUkMJZvL+WtFbv43Pi+JCWEuOuPC3hvze4WfXdMvxwWeFf5/PHWM6iorqV3TjqLtu6nf146pw/sGsXKo0MBLyKBFQ47yg7W8sqibVw+shdpyQls2VvBa0t38vrynWzdV8n+ipoWrSsxZNSGHUMKMvncuL70yknjvKHdO/UJXQW8iMS14rIqXlm4jQWb93PF6F68ungHry7a3qp1PH7dWC4aXtBwV25tXbhTDMmggBcRacQ5R3F5FWf/aBZfGN+X3324CYDJQ/N5e1Vxs98d1TcHgEVb9pOWlEDP7FT+cvtE6sIOjA6/mkcBLyLSjH0HqtlfWcOAbhkAlB2s4Z3VxXz9TwupqXNcNLygxc/IvfdTw3h3dTEfrNvDNy8aQu/cND51ak8O1tSRnZbU7uPyKOBFRNooHHaEQsa8jXv5cN0eVu0qY8biHQAMKchk9a7yVq0vNz2JH149kvW7y5m3cR83TxpAdloSp/Tq0qbwV8CLiLSjt1cVcVL3TPrkpgPw5vJdfPn38zh9QB4TBuSxv6KGZ+dsatU6N/zwEgW8iEhnVF0bJjnx0ElX5xyllbXM9Vr+F57cnczURPYcqObGpz8GIDkx1DDYWlufhavhgkVEoqxxuAOYGdnpSUwZXsCU4YffVbvgO1M4UF1Ln9x0fvfBRob1yIpKTQp4EZEOlpuRTG5GZEz8G84sjNp2/L+IU0REokIBLyISUAp4EZGAUsCLiASUAl5EJKAU8CIiAaWAFxEJKAW8iEhAdaqhCsysGGjdAA6HdANa9liX4IjHfYb43G/tc/xo7X73d87lN/VBpwr4E2Fm8441HkNQxeM+Q3zut/Y5frTnfquLRkQkoBTwIiIBFaSAf8LvAnwQj/sM8bnf2uf40W77HZg+eBEROVyQWvAiItKIAl5EJKBiPuDNbKqZrTKztWZ2r9/1tCcz62tms8xsuZktM7O7vPl5ZvaGma3x/s315puZ/dz7WSw2s9P83YO2M7MEM1tgZjO86QFm9pG3b38ys2Rvfoo3vdb7vNDXwtvIzHLM7EUzW2lmK8xsYpwc5697/7eXmtkLZpYatGNtZr81syIzW9poXquPrZnd4C2/xsxuaMm2YzrgzSwBeAz4FDAcuMbMhvtbVbuqBb7hnBsOnAF81du/e4G3nHODgbe8aYj8HAZ7r1uBX3V8ye3mLmBFo+kfAT9zzp0E7ANu9ubfDOzz5v/MWy4W/S/wmnNuGDCKyL4H+jibWW/gTmCcc+5UIAH4AsE71s8AU4+Y16pja2Z5wAPA6cAE4IH6XwrNcs7F7AuYCLzeaPo+4D6/64ri/r4CTAFWAT29eT2BVd77x4FrGi3fsFwsvYA+3n/684EZgBG5sy/xyOMOvA5M9N4nesuZ3/vQyv3NBjYcWXccHOfewBYgzzt2M4CLg3isgUJgaVuPLXAN8Hij+Yctd6xXTLfgOfQfpN5Wb17geH+OjgE+Agqcczu8j3YC9U/0DcrP43+Ae4CwN90V2O+cq/WmG+9Xwz57n5d4y8eSAUAx8LTXLfUbM8sg4MfZObcN+AmwGdhB5NjNJ9jHul5rj22bjnmsB3xcMLNM4K/A3c650safuciv88Bc62pmlwFFzrn5ftfSgRKB04BfOefGAAc49Cc7ELzjDOB1MVxB5BdcLyCDo7syAi+axzbWA34b0LfRdB9vXmCYWRKRcH/OOfeSN3uXmfX0Pu8JFHnzg/DzOAv4tJltBP5IpJvmf4EcM0v0lmm8Xw377H2eDezpyILbwVZgq3PuI2/6RSKBH+TjDHAhsME5V+ycqwFeInL8g3ys67X22LbpmMd6wH8MDPbOuicTOUEz3eea2o2ZGfAUsMI592ijj6YD9WfRbyDSN18//3rvTPwZQEmjPwNjgnPuPudcH+dcIZHj+S/n3L8Bs4DPeosduc/1P4vPesvHVEvXObcT2GJmQ71ZFwDLCfBx9mwGzjCzdO//ev1+B/ZYN9LaY/s6cJGZ5Xp/+VzkzWue3ycf2uHkxSXAamAd8C2/62nnfZtE5E+3xcBC73UJkX7Ht4A1wJtAnre8EbmqaB2whMjVCb7vxwns/2Rghvd+IDAXWAv8BUjx5qd602u9zwf6XXcb93U0MM871i8DufFwnIHvAiuBpcCzQErQjjXwApFzDDVE/lq7uS3HFrjJ2/e1wI0t2baGKhARCahY76IREZFjUMCLiASUAl5EJKAU8CIiAaWAFxEJKAW8BIaZlXv/FprZte287vuPmP6gPdcvEg0KeAmiQqBVAd/ozsljOSzgnXNntrImkQ6ngJcgegQ428wWeuONJ5jZf5vZx94Y27cBmNlkM3vPzKYTuYMSM3vZzOZ7Y5Tf6s17BEjz1vecN6/+rwXz1r3UzJaY2ecbrfttOzTG+3Pe3ZqY2SMWGeN/sZn9pMN/OhI3jtdqEYlF9wLfdM5dBuAFdYlzbryZpQDvm9k/vWVPA051zm3wpm9yzu01szTgYzP7q3PuXjP7mnNudBPbuprIXaijgG7ed971PhsDnAJsB94HzjKzFcBVwDDnnDOznPbddZFD1IKXeHARkfE9FhIZbrkrkQcqAMxtFO4Ad5rZImAOkcGdBtO8ScALzrk659wu4B1gfKN1b3XOhYkMM1FIZIjbg8BTZnY1UHGC+yZyTAp4iQcG3OGcG+29Bjjn6lvwBxoWMptMZITDic65UcACIuOftFVVo/d1RB5iUUvkiTwvApcBr53A+kWapYCXICoDshpNvw78uzf0MmY2xHugxpGyiTwSrsLMhhF5TGK9mvrvH+E94PNeP38+cA6RgbCa5I3tn+2cmwl8nUjXjkhUqA9egmgxUOd1tTxDZDz5QuAT70RnMXBlE997Dbjd6ydfRaSbpt4TwGIz+8RFhi+u9zcij5VbRGTkz3ucczu9XxBNyQJeMbNUIn9ZTGvTHoq0gEaTFBEJKHXRiIgElAJeRCSgFPAiIgGlgBcRCSgFvIhIQCngRUQCSgEvIhJQ/wf3XPRvi4neDwAAAABJRU5ErkJggg=="
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "id": "F31vzJ_u66cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Generate music using the RNN model\n",
        "\n",
        "Now, we can use our trained RNN model to generate some music! When generating music, we'll have to feed the model some sort of seed to get it started (because it can't predict anything without something to start with!).\n",
        "\n",
        "Once we have a generated seed, we can then iteratively predict each successive character (remember, we are using the ABC representation for our music) using our trained RNN. More specifically, recall that our RNN outputs a `softmax` over possible successive characters. For inference, we iteratively sample from these distributions, and then use our samples to encode a generated song in the ABC format.\n",
        "\n",
        "Then, all we have to do is write it to a file and listen!"
      ],
      "metadata": {
        "id": "kKkD5M6eoSiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restore the latest checkpoint\n",
        "\n",
        "To keep this inference step simple, we will use a batch size of 1. Because of how the RNN state is passed from timestep to timestep, the model will only be able to accept a fixed batch size once it is built. \n",
        "\n",
        "To run the model with a different `batch_size`, we'll need to rebuild the model and restore the weights from the latest checkpoint, i.e., the weights after the last checkpoint during training:"
      ],
      "metadata": {
        "id": "JIPcXllKjkdr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "source": [
        "'''TODO: Rebuild the model using a batch_size=1'''\r\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\r\n",
        "\r\n",
        "# Restore the model weights for the last checkpoint after training\r\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n",
        "model.build(tf.TensorShape([1, None]))\r\n",
        "\r\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (1, None, 256)            21248     \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (1, None, 83)             85075     \n",
            "=================================================================\n",
            "Total params: 5,353,299\n",
            "Trainable params: 5,353,299\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "LycQ-ot_jjyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we have fed in a fixed `batch_size` of 1 for inference."
      ],
      "metadata": {
        "id": "I9b4V2C8N62l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The prediction procedure\n",
        "\n",
        "Now, we're ready to write the code to generate text in the ABC music format:\n",
        "\n",
        "* Initialize a \"seed\" start string and the RNN state, and set the number of characters we want to generate.\n",
        "\n",
        "* Use the start string and the RNN state to obtain the probability distribution over the next predicted character.\n",
        "\n",
        "* Sample from multinomial distribution to calculate the index of the predicted character. This predicted character is then used as the next input to the model.\n",
        "\n",
        "* At each time step, the updated RNN state is fed back into the model, so that it now has more context in making the next prediction. After predicting the next character, the updated RNN states are again fed back into the model, which is how it learns sequence dependencies in the data, as it gets more information from the previous predictions.\n",
        "\n",
        "![LSTM inference](https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab1/img/lstm_inference.png)\n",
        "\n",
        "Complete and experiment with this code block (as well as some of the aspects of network definition and training!), and see how the model performs. How do songs generated after training with a small number of epochs compare to those generated after a longer duration of training?"
      ],
      "metadata": {
        "id": "DjGz1tDkzf-u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "source": [
        "### Prediction of a generated song ###\r\n",
        "\r\n",
        "def generate_text(model, start_string, generation_length=1000):\r\n",
        "  # Evaluation step (generating ABC text using the learned RNN model)\r\n",
        "\r\n",
        "  '''TODO: convert the start string to numbers (vectorize)'''\r\n",
        "  input_eval = vectorize_string(start_string)\r\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\r\n",
        "\r\n",
        "  # Empty string to store our results\r\n",
        "  text_generated = []\r\n",
        "\r\n",
        "  # Here batch size == 1\r\n",
        "  model.reset_states()\r\n",
        "  tqdm._instances.clear()\r\n",
        "\r\n",
        "  for i in tqdm(range(generation_length)):\r\n",
        "      '''TODO: evaluate the inputs and generate the next character predictions'''\r\n",
        "      predictions = model(input_eval)\r\n",
        "      \r\n",
        "      # Remove the batch dimension\r\n",
        "      predictions = tf.squeeze(predictions, 0)\r\n",
        "      \r\n",
        "      '''TODO: use a multinomial distribution to sample'''\r\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n",
        "      \r\n",
        "      # Pass the prediction along with the previous hidden state\r\n",
        "      #   as the next inputs to the model\r\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\r\n",
        "      \r\n",
        "      '''TODO: add the predicted character to the generated text!'''\r\n",
        "      # Hint: consider what format the prediction is in vs. the output\r\n",
        "      text_generated.append(idx2char[predicted_id])\r\n",
        "    \r\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "outputs": [],
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "source": [
        "'''TODO: Use the model and the function defined above to generate ABC format text of length 1000!\r\n",
        "    As you may notice, ABC files start with \"X\" - this may be a good start string.'''\r\n",
        "generated_text = generate_text(model, start_string=\"X\", generation_length=1000) # TODO\r\n",
        "# generated_text = generate_text('''TODO''', start_string=\"X\", generation_length=1000)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 1000/1000 [00:03<00:00, 251.54it/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "ktovv0RFhrkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Play back the generated music!\n",
        "\n",
        "We can now call a function to convert the ABC format text to an audio file, and then play that back to check out our generated music! Try training longer if the resulting song is not long enough, or re-generating the song!"
      ],
      "metadata": {
        "id": "AM2Uma_-yVIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "source": [
        "### Play back generated songs ###\r\n",
        "\r\n",
        "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\r\n",
        "\r\n",
        "for i, song in enumerate(generated_songs): \r\n",
        "  print(song + \"\\n\")\r\n",
        "  \r\n",
        "  # Synthesize the waveform from a song\r\n",
        "  waveform = mdl.lab1.play_song(song)\r\n",
        "\r\n",
        "  # If its a valid song (correct syntax), lets play it! \r\n",
        "  if waveform:\r\n",
        "    print(\"Generated song\", i)\r\n",
        "    ipythondisplay.display(waveform)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 songs in text\n",
            "XZ:17\n",
            "T:T:vpHile Coynny\n",
            "Z: id:dc-jig-337\n",
            "M:6/8\n",
            "L:1/8\n",
            "K:D Major\n",
            "A|DFA dAF|EFG EAB|GBd cBA|GFG AFD|!\n",
            "GFG AGF|GBg ABc|dfe dcA|AGF G2:|!\n",
            "e|fdd ced|fdf gec|agf gde|fd^c d2:|!\n",
            "\n",
            "X:133\n",
            "T:Sing thaing of the Waneo\n",
            "Z: id:dc-jig-115\n",
            "M:6/8\n",
            "L:1/8\n",
            "K:D Major\n",
            "G|F2D DEC|DED KA,D|F3 D2c|!\n",
            "BAG GEC|ED G3 AD|G3 GE DED|!\n",
            "G2D G2A|D2E FAG|EDD ADF|DED D2:|!\n",
            "K:|D F3A|dFFD ADFA|f2df e2de|fedB AFdf|!\n",
            "eaed Bded|cBAG ABGA|B2AG FEDg|!\n",
            "fafd efdB|AFEF DEFA|Bgfe dBfb|afef ed:|!\n",
            "Bc|dAfA GABd|(3Bcd ef geaf|ebaf gfef|(3baf (3gaf (3gfe (3dedB|ADFA B2Ac|!\n",
            "BGBG DGBG|EAA2 cABG|E2DG EGBG|AGFA G2:|!\n",
            "D|GBB2d|e2fe dBAF|D3A BGEF|D2AF DFAF|!\n",
            "G2GA BGG2|ABE2 cBAG|BddB AGFA|G2FG EGE:|!\n",
            "\n",
            "X:66\n",
            "T:Dunker Briont\n",
            "Z: id:dc-reel-36\n",
            "M:C\n",
            "L:1/8\n",
            "K:D Mixolydian\n",
            "Adc|AdcA GAcA|G2G2 G2C2|D2DE D2GA|BGG2 AGEG|!\n",
            "FGAB c2AG|F2DE FGAB|c2BG AGFG|AdGF G2:|!\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "LrOtG64bfLto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Experiment and **get awarded for the best songs**!\n",
        "\n",
        "Congrats on making your first sequence model in TensorFlow! It's a pretty big accomplishment, and hopefully you have some sweet tunes to show for it.\n",
        "\n",
        "Consider how you may improve your model and what seems to be most important in terms of performance. Here are some ideas to get you started:\n",
        "\n",
        "*  How does the number of training epochs affect the performance?\n",
        "*  What if you alter or augment the dataset? \n",
        "*  Does the choice of start string significantly affect the result? \n",
        "\n",
        "Try to optimize your model and submit your best song! **MIT students and affiliates will be eligible for prizes during the IAP offering**. To enter the competition, MIT students and affiliates should upload the following to the course Canvas:\n",
        "\n",
        "* a recording of your song;\n",
        "* iPython notebook with the code you used to generate the song;\n",
        "* a description and/or diagram of the architecture and hyperparameters you used -- if there are any additional or interesting modifications you made to the template code, please include these in your description.\n",
        "\n",
        "You can also tweet us at [@MITDeepLearning](https://twitter.com/MITDeepLearning) a copy of the song! See this example song generated by a previous 6.S191 student (credit Ana Heart): <a href=\"https://twitter.com/AnaWhatever16/status/1263092914680410112?s=20\">song from May 20, 2020.</a>\n",
        "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
        "\n",
        "Have fun and happy listening!\n",
        "\n",
        "![Let's Dance!](http://33.media.tumblr.com/3d223954ad0a77f4e98a7b87136aa395/tumblr_nlct5lFVbF1qhu7oio1_500.gif)"
      ],
      "metadata": {
        "id": "HgVvcrYmSKGG"
      }
    }
  ]
}